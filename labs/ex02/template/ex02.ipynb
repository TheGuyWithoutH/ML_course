{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.06780585492638"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    # error = y - tx.dot(w)\n",
    "    # return  np.sum(error**2) / (2 * len(y))\n",
    "\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MAE\n",
    "    # ***************************************************\n",
    "    error = y - tx.dot(w)\n",
    "    return np.sum(np.abs(error)) / len(y)\n",
    "\n",
    "compute_loss(y, tx, np.array([1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "    \n",
    "    for i, w0 in enumerate(grid_w0):\n",
    "        for j, w1 in enumerate(grid_w1):\n",
    "            losses[i, j] = compute_loss(y, tx, np.array([w0, w1]))\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=16.41638060128349, w0*=71.42857142857142, w1*=15.306122448979579, execution time=0.010 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=50)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************\n",
    "    return -tx.T.dot(y - tx.dot(w)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * gradient\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2829.2722244384163, w0=51.54259072181181, w1=10.132993413506076\n",
      "GD iter. 1/49: loss=267.05002587794183, w0=67.00536793835533, w1=13.172891437557823\n",
      "GD iter. 2/49: loss=36.450028007500265, w0=71.64420110331838, w1=14.084860844773322\n",
      "GD iter. 3/49: loss=15.696028199160635, w0=73.03585105280729, w1=14.358451666937965\n",
      "GD iter. 4/49: loss=13.828168216410077, w0=73.45334603765397, w1=14.440528913587356\n",
      "GD iter. 5/49: loss=13.660060817962522, w0=73.57859453310797, w1=14.46515208758217\n",
      "GD iter. 6/49: loss=13.644931152102242, w0=73.61616908174418, w1=14.472539039780616\n",
      "GD iter. 7/49: loss=13.643569482174817, w0=73.62744144633503, w1=14.474755125440149\n",
      "GD iter. 8/49: loss=13.643446931881353, w0=73.63082315571229, w1=14.47541995113801\n",
      "GD iter. 9/49: loss=13.643435902354941, w0=73.63183766852546, w1=14.475619398847368\n",
      "GD iter. 10/49: loss=13.64343490969756, w0=73.63214202236942, w1=14.475679233160175\n",
      "GD iter. 11/49: loss=13.643434820358397, w0=73.6322333285226, w1=14.475697183454017\n",
      "GD iter. 12/49: loss=13.643434812317876, w0=73.63226072036856, w1=14.47570256854217\n",
      "GD iter. 13/49: loss=13.64343481159423, w0=73.63226893792235, w1=14.475704184068615\n",
      "GD iter. 14/49: loss=13.643434811529096, w0=73.63227140318848, w1=14.475704668726548\n",
      "GD iter. 15/49: loss=13.643434811523234, w0=73.63227214276833, w1=14.47570481412393\n",
      "GD iter. 16/49: loss=13.643434811522706, w0=73.63227236464228, w1=14.475704857743143\n",
      "GD iter. 17/49: loss=13.64343481152266, w0=73.63227243120447, w1=14.475704870828908\n",
      "GD iter. 18/49: loss=13.643434811522654, w0=73.63227245117312, w1=14.475704874754637\n",
      "GD iter. 19/49: loss=13.643434811522656, w0=73.63227245716372, w1=14.475704875932356\n",
      "GD iter. 20/49: loss=13.643434811522653, w0=73.6322724589609, w1=14.475704876285672\n",
      "GD iter. 21/49: loss=13.643434811522656, w0=73.63227245950004, w1=14.475704876391665\n",
      "GD iter. 22/49: loss=13.643434811522656, w0=73.63227245966179, w1=14.475704876423464\n",
      "GD iter. 23/49: loss=13.643434811522654, w0=73.63227245971032, w1=14.475704876433003\n",
      "GD iter. 24/49: loss=13.643434811522656, w0=73.63227245972487, w1=14.475704876435865\n",
      "GD iter. 25/49: loss=13.643434811522656, w0=73.63227245972924, w1=14.475704876436724\n",
      "GD iter. 26/49: loss=13.643434811522654, w0=73.63227245973054, w1=14.475704876436982\n",
      "GD iter. 27/49: loss=13.643434811522654, w0=73.63227245973094, w1=14.47570487643706\n",
      "GD iter. 28/49: loss=13.643434811522656, w0=73.63227245973107, w1=14.475704876437083\n",
      "GD iter. 29/49: loss=13.643434811522654, w0=73.6322724597311, w1=14.475704876437089\n",
      "GD iter. 30/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.47570487643709\n",
      "GD iter. 31/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 32/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 33/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 34/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 35/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 36/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 37/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 38/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 39/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 40/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 41/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 42/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 43/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 44/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 45/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 46/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 47/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 48/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 49/49: loss=13.643434811522654, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD: execution time=0.001 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea35f5844d04b52ab325edbd22f40a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # ***************************************************\n",
    "    return compute_gradient(y, tx, w)\n",
    "    \n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    batches = batch_iter(y, tx, batch_size, num_batches=max_iters)\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        batch_y, batch_tx = next(batches)\n",
    "        gradient = compute_stoch_gradient(batch_y, batch_tx, w)\n",
    "        w = w - gamma * gradient\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2829.2722244384163, w0=7.514363177464874, w1=-0.7514847631153557\n",
      "SGD iter. 1/49: loss=2315.3660508999656, w0=12.46462529568357, w1=-1.932239824966115\n",
      "SGD iter. 2/49: loss=2018.9942892663844, w0=18.88627124110029, w1=2.0628359298174934\n",
      "SGD iter. 3/49: loss=1589.2454172696644, w0=25.902065136166343, w1=13.45932339395518\n",
      "SGD iter. 4/49: loss=1153.246296045727, w0=30.20133190712407, w1=10.36839192248649\n",
      "SGD iter. 5/49: loss=965.2017433044116, w0=33.72475973205279, w1=6.710416722696875\n",
      "SGD iter. 6/49: loss=840.098070921736, w0=36.382275530004804, w1=3.045641465345015\n",
      "SGD iter. 7/49: loss=772.7477452346263, w0=41.20948686081488, w1=4.311861099263755\n",
      "SGD iter. 8/49: loss=590.9138079715611, w0=45.240168848644714, w1=7.497125965810135\n",
      "SGD iter. 9/49: loss=441.0494903487795, w0=49.683802292434244, w1=14.40768417091926\n",
      "SGD iter. 10/49: loss=300.4103598966663, w0=52.99057602189531, w1=16.5507148129492\n",
      "SGD iter. 11/49: loss=228.83608384571627, w0=54.76316041075601, w1=14.96112271365687\n",
      "SGD iter. 12/49: loss=191.78294480825699, w0=56.94979554426926, w1=17.389567196023744\n",
      "SGD iter. 13/49: loss=157.04124963773486, w0=58.27553732654847, w1=16.46197348879887\n",
      "SGD iter. 14/49: loss=133.5307232871124, w0=60.92595658244967, w1=13.229655192618608\n",
      "SGD iter. 15/49: loss=95.14498630542201, w0=61.69522491724363, w1=13.992885510468904\n",
      "SGD iter. 16/49: loss=85.00654409740291, w0=63.12491622860865, w1=14.534993177602116\n",
      "SGD iter. 17/49: loss=68.84745984670423, w0=64.39611104413343, w1=13.257634367861153\n",
      "SGD iter. 18/49: loss=57.03862154094158, w0=64.91095610844825, w1=12.573672937666307\n",
      "SGD iter. 19/49: loss=53.482977009151725, w0=66.24935706833412, w1=10.309382192992548\n",
      "SGD iter. 20/49: loss=49.576277001078324, w0=65.8643492514869, w1=10.591396466607133\n",
      "SGD iter. 21/49: loss=51.35767620745014, w0=66.1293055209714, w1=10.302867427263264\n",
      "SGD iter. 22/49: loss=50.49697744219716, w0=67.09075958160155, w1=10.779459820966286\n",
      "SGD iter. 23/49: loss=41.8702439339364, w0=67.88482367392263, w1=10.020223492743462\n",
      "SGD iter. 24/49: loss=40.0856757644887, w0=67.64199188634187, w1=10.302590283765827\n",
      "SGD iter. 25/49: loss=40.29260818726793, w0=67.75356911846498, w1=10.087939654769308\n",
      "SGD iter. 26/49: loss=40.54925311906818, w0=69.05074274414801, w1=11.291122130237689\n",
      "SGD iter. 27/49: loss=29.20942571260365, w0=69.76206232551951, w1=12.448298550217958\n",
      "SGD iter. 28/49: loss=23.18788625879646, w0=70.01422895691455, w1=12.1187892032492\n",
      "SGD iter. 29/49: loss=22.96607995091862, w0=70.89493549093986, w1=11.47571849588214\n",
      "SGD iter. 30/49: loss=21.889900793635924, w0=71.30594100221626, w1=11.563650784665443\n",
      "SGD iter. 31/49: loss=20.589373353336278, w0=73.30951870418096, w1=15.398683370932613\n",
      "SGD iter. 32/49: loss=14.121464455534129, w0=73.5683548084287, w1=15.334025061942297\n",
      "SGD iter. 33/49: loss=14.013834315019508, w0=73.46431091083306, w1=15.485545813829736\n",
      "SGD iter. 34/49: loss=14.167429711893796, w0=73.34122706803424, w1=15.320155330255913\n",
      "SGD iter. 35/49: loss=14.042336806014054, w0=73.48288145045247, w1=15.505350751315676\n",
      "SGD iter. 36/49: loss=14.184678962176545, w0=73.76186508446096, w1=15.335760216512627\n",
      "SGD iter. 37/49: loss=14.021679529711063, w0=72.85969126592347, w1=15.767539249513856\n",
      "SGD iter. 38/49: loss=14.776293685766591, w0=72.1956205035556, w1=16.515636030592788\n",
      "SGD iter. 39/49: loss=16.7560787899616, w0=72.1068219520235, w1=16.479383711887564\n",
      "SGD iter. 40/49: loss=16.81429887507145, w0=72.9877345646206, w1=16.824551492349794\n",
      "SGD iter. 41/49: loss=16.60968957318168, w0=73.5653630014567, w1=16.936319367950343\n",
      "SGD iter. 42/49: loss=16.67298508724845, w0=73.5091224437694, w1=16.929230946484036\n",
      "SGD iter. 43/49: loss=16.66091286293834, w0=74.69281831838681, w1=17.51740505405779\n",
      "SGD iter. 44/49: loss=18.83178355594743, w0=74.4295497221902, w1=17.434681129479934\n",
      "SGD iter. 45/49: loss=18.33903056117552, w0=74.26272742084434, w1=17.53704394860672\n",
      "SGD iter. 46/49: loss=18.528069997914994, w0=75.20377081817234, w1=16.96973195144232\n",
      "SGD iter. 47/49: loss=17.98832388224397, w0=75.56683204104588, w1=16.96723386824637\n",
      "SGD iter. 48/49: loss=18.61855355686414, w0=76.75962353636073, w1=14.96927347280873\n",
      "SGD iter. 49/49: loss=18.655402169433014, w0=76.21874404536614, w1=15.167549287656119\n",
      "SGD: execution time=0.002 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0389939147e4801a982b93954e6cbe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.847464098448484, w1=7.724426406192441\n",
      "GD iter. 1/49: loss=318.282124701595, w0=67.401703327983, w1=10.041754328050121\n",
      "GD iter. 2/49: loss=88.6423556165126, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=67.97477639885521, w0=73.46785662750146, w1=10.945512217574594\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631796\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.05160722578589, w1=11.03248153448191\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249236, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038407\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003893\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225756, w1=11.034889001593537\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608876, w1=11.034894818487498\n",
      "GD iter. 16/49: loss=65.93073010260342, w0=74.06780575927509, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706558\n",
      "GD iter. 19/49: loss=65.93073010260339, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873671\n",
      "GD iter. 21/49: loss=65.93073010260338, w0=74.06780585469393, w1=11.03489486595447\n",
      "GD iter. 22/49: loss=65.93073010260336, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260336, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.034894865988818\n",
      "GD iter. 26/49: loss=65.93073010260338, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=65.93073010260336, w0=74.06780585492619, w1=11.034894865989077\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989095\n",
      "GD iter. 29/49: loss=65.93073010260338, w0=74.06780585492635, w1=11.034894865989099\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.034894865989102\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.001 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "sgd_losses, sgd_ws = gradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma\n",
    ")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95249baddfe4441ba1cb7db68445d337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************\n",
    "    error = y - tx.dot(w)\n",
    "    return -tx.T.dot(np.sign(error)) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        gradient = compute_subgradient_mae(y, tx, w)\n",
    "\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * gradient\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=8.756471895211877e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.7512943790423754e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=2.626941568563563e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492637, w0=2.8, w1=3.502588758084751e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=4.378235947605939e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492638, w0=4.2, w1=5.253883137127127e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=6.1295303266483146e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=7.0051775161695025e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=7.88082470569069e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=8.756471895211878e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=9.632119084733065e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=1.0507766274254253e-14\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=1.1383413463775441e-14\n",
      "SubGD iter. 13/499: loss=64.96780585492637, w0=9.799999999999999, w1=1.2259060653296629e-14\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=1.3134707842817817e-14\n",
      "SubGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=1.4010355032339005e-14\n",
      "SubGD iter. 16/499: loss=62.86780585492639, w0=11.899999999999997, w1=1.488600222186019e-14\n",
      "SubGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.576164941138138e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.6637296600902567e-14\n",
      "SubGD iter. 19/499: loss=60.767805854926394, w0=13.999999999999995, w1=1.7512943790423755e-14\n",
      "SubGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.8388590979944943e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.926423816946613e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=2.013988535898732e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=2.1015532548508507e-14\n",
      "SubGD iter. 24/499: loss=57.267805854926394, w0=17.499999999999993, w1=2.1891179738029695e-14\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=2.2766826927550882e-14\n",
      "SubGD iter. 26/499: loss=55.86780585492639, w0=18.89999999999999, w1=2.364247411707207e-14\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=2.4518121306593258e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=2.5393768496114446e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=2.6269415685635634e-14\n",
      "SubGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=2.7145062875156822e-14\n",
      "SubGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=2.802071006467801e-14\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.8896357254199195e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492638, w0=23.799999999999986, w1=2.977200444372038e-14\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=3.064765163324157e-14\n",
      "SubGD iter. 35/499: loss=49.567805854926405, w0=25.199999999999985, w1=3.152329882276276e-14\n",
      "SubGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=3.2398946012283946e-14\n",
      "SubGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=3.3274593201805134e-14\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=3.415024039132632e-14\n",
      "SubGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=3.502588758084751e-14\n",
      "SubGD iter. 40/499: loss=46.06780585492639, w0=28.69999999999998, w1=3.59015347703687e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=3.6777181959889886e-14\n",
      "SubGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=3.7652829149411074e-14\n",
      "SubGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=3.852847633893226e-14\n",
      "SubGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=3.940412352845345e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=4.027977071797464e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=4.1155417907495825e-14\n",
      "SubGD iter. 47/499: loss=41.1678058549264, w0=33.59999999999999, w1=4.2031065097017013e-14\n",
      "SubGD iter. 48/499: loss=40.4678058549264, w0=34.29999999999999, w1=4.29067122865382e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=4.378235947605939e-14\n",
      "SubGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=4.465800666558058e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492639, w0=36.4, w1=4.5533653855101765e-14\n",
      "SubGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=4.640930104462295e-14\n",
      "SubGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=4.728494823414414e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=4.816059542366533e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=4.9036242613186517e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=4.9911889802707705e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=5.078753699222889e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=5.166318418175008e-14\n",
      "SubGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=5.253883137127127e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=5.3414478560792456e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=5.4290125750313644e-14\n",
      "SubGD iter. 62/499: loss=30.667805854926346, w0=44.10000000000003, w1=5.516577293983483e-14\n",
      "SubGD iter. 63/499: loss=29.967805854926347, w0=44.80000000000003, w1=5.604142012935602e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=5.691706731887721e-14\n",
      "SubGD iter. 65/499: loss=28.567805854926338, w0=46.20000000000004, w1=5.779271450839839e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=5.866836169791957e-14\n",
      "SubGD iter. 67/499: loss=27.173270209668917, w0=47.59306930693074, w1=0.01114784567828894\n",
      "SubGD iter. 68/499: loss=26.4904515637512, w0=48.279207920792125, w1=0.03308574108991741\n",
      "SubGD iter. 69/499: loss=25.81721232277017, w0=48.96534653465351, w1=0.055023636501545875\n",
      "SubGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.1053832638830964\n",
      "SubGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532795278\n",
      "SubGD iter. 72/499: loss=23.899295346035586, w0=50.947524752475296, w1=0.22954810677280915\n",
      "SubGD iter. 73/499: loss=23.28439292565714, w0=51.59207920792084, w1=0.312425129327494\n",
      "SubGD iter. 74/499: loss=22.68687644418184, w0=52.22277227722777, w1=0.41195013288401805\n",
      "SubGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923948\n",
      "SubGD iter. 76/499: loss=21.53781882800843, w0=53.4564356435644, w1=0.6457900912636185\n",
      "SubGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577408\n",
      "SubGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995888\n",
      "SubGD iter. 79/499: loss=19.91191015895784, w0=55.24455445544559, w1=1.067092029785011\n",
      "SubGD iter. 80/499: loss=19.389644090563227, w0=55.819801980198065, w1=1.2261255948210965\n",
      "SubGD iter. 81/499: loss=18.887989064395878, w0=56.36732673267331, w1=1.410709342622233\n",
      "SubGD iter. 82/499: loss=18.41596050185423, w0=56.900990099009945, w1=1.605853732220289\n",
      "SubGD iter. 83/499: loss=17.954898543040382, w0=57.42772277227727, w1=1.808762802293982\n",
      "SubGD iter. 84/499: loss=17.505757656579817, w0=57.933663366336674, w1=2.0285064197514897\n",
      "SubGD iter. 85/499: loss=17.074957426931608, w0=58.43267326732677, w1=2.2494370848672975\n",
      "SubGD iter. 86/499: loss=16.652967297509893, w0=58.91089108910895, w1=2.4837982986028537\n",
      "SubGD iter. 87/499: loss=16.248540731496718, w0=59.382178217821824, w1=2.7260245553531703\n",
      "SubGD iter. 88/499: loss=15.849105212654152, w0=59.83960396039608, w1=2.978742333469156\n",
      "SubGD iter. 89/499: loss=15.466919791231321, w0=60.262376237623805, w1=3.251528669355458\n",
      "SubGD iter. 90/499: loss=15.108294621512211, w0=60.67821782178222, w1=3.5270865794243\n",
      "SubGD iter. 91/499: loss=14.754896345922827, w0=61.087128712871326, w1=3.806459183951836\n",
      "SubGD iter. 92/499: loss=14.404528961620272, w0=61.49603960396043, w1=4.085831788479371\n",
      "SubGD iter. 93/499: loss=14.05578702812727, w0=61.891089108910926, w1=4.373839384328629\n",
      "SubGD iter. 94/499: loss=13.714620911605627, w0=62.27920792079211, w1=4.666037469532069\n",
      "SubGD iter. 95/499: loss=13.381236307284146, w0=62.65346534653469, w1=4.959829093241791\n",
      "SubGD iter. 96/499: loss=13.058821615166227, w0=63.02079207920796, w1=5.257057192056662\n",
      "SubGD iter. 97/499: loss=12.740251724339231, w0=63.38118811881192, w1=5.5604343163524295\n",
      "SubGD iter. 98/499: loss=12.423218888756102, w0=63.74158415841588, w1=5.863811440648197\n",
      "SubGD iter. 99/499: loss=12.107561731901159, w0=64.08811881188123, w1=6.172402175278572\n",
      "SubGD iter. 100/499: loss=11.800622097398126, w0=64.42772277227726, w1=6.4863693105165225\n",
      "SubGD iter. 101/499: loss=11.495041794646415, w0=64.7673267326733, w1=6.800336445754473\n",
      "SubGD iter. 102/499: loss=11.189461491894704, w0=65.10693069306933, w1=7.1143035809924235\n",
      "SubGD iter. 103/499: loss=10.883881189142992, w0=65.44653465346536, w1=7.428270716230374\n",
      "SubGD iter. 104/499: loss=10.58459340831319, w0=65.76534653465349, w1=7.747893210218651\n",
      "SubGD iter. 105/499: loss=10.295816534318933, w0=66.070297029703, w1=8.073669686866932\n",
      "SubGD iter. 106/499: loss=10.01135208122135, w0=66.37524752475251, w1=8.399446163515213\n",
      "SubGD iter. 107/499: loss=9.728084326668117, w0=66.6663366336634, w1=8.73297028041742\n",
      "SubGD iter. 108/499: loss=9.448125461122496, w0=66.9574257425743, w1=9.066494397319628\n",
      "SubGD iter. 109/499: loss=9.171041104096656, w0=67.23465346534658, w1=9.398630319470323\n",
      "SubGD iter. 110/499: loss=8.903656131158947, w0=67.51188118811886, w1=9.730766241621017\n",
      "SubGD iter. 111/499: loss=8.63627115822124, w0=67.78910891089114, w1=10.062902163771712\n",
      "SubGD iter. 112/499: loss=8.376151920302359, w0=68.06633663366343, w1=10.363999289979459\n",
      "SubGD iter. 113/499: loss=8.140540838751482, w0=68.32970297029709, w1=10.66046690927365\n",
      "SubGD iter. 114/499: loss=7.918544501597259, w0=68.59306930693076, w1=10.943174379960851\n",
      "SubGD iter. 115/499: loss=7.7052797283769845, w0=68.85643564356442, w1=11.225881850648053\n",
      "SubGD iter. 116/499: loss=7.493695831178626, w0=69.11287128712878, w1=11.504395843582245\n",
      "SubGD iter. 117/499: loss=7.2899924057434, w0=69.35544554455453, w1=11.78820189306779\n",
      "SubGD iter. 118/499: loss=7.097234035781528, w0=69.58415841584166, w1=12.06091146519101\n",
      "SubGD iter. 119/499: loss=6.919905294668907, w0=69.80594059405948, w1=12.324245668386087\n",
      "SubGD iter. 120/499: loss=6.7505735273154395, w0=70.0277227722773, w1=12.587579871581164\n",
      "SubGD iter. 121/499: loss=6.584744810805652, w0=70.25643564356443, w1=12.824765405096523\n",
      "SubGD iter. 122/499: loss=6.4303432763477915, w0=70.47821782178225, w1=13.065616959310187\n",
      "SubGD iter. 123/499: loss=6.27807148189034, w0=70.69306930693077, w1=13.302953389983951\n",
      "SubGD iter. 124/499: loss=6.133663329263311, w0=70.89405940594067, w1=13.525403099312957\n",
      "SubGD iter. 125/499: loss=6.005840798343018, w0=71.08811881188126, w1=13.742945617944251\n",
      "SubGD iter. 126/499: loss=5.885021825223206, w0=71.27524752475254, w1=13.953548196006883\n",
      "SubGD iter. 127/499: loss=5.771635252269647, w0=71.46237623762383, w1=14.164150774069515\n",
      "SubGD iter. 128/499: loss=5.667162061790248, w0=71.62178217821788, w1=14.349779559473214\n",
      "SubGD iter. 129/499: loss=5.586726765993136, w0=71.75346534653471, w1=14.516890107612351\n",
      "SubGD iter. 130/499: loss=5.523847812160378, w0=71.87128712871292, w1=14.670791185324227\n",
      "SubGD iter. 131/499: loss=5.480093708591866, w0=71.95445544554461, w1=14.780276456654562\n",
      "SubGD iter. 132/499: loss=5.453088003502018, w0=72.0376237623763, w1=14.889761727984897\n",
      "SubGD iter. 133/499: loss=5.4273926308629, w0=72.10693069306937, w1=14.985916181776767\n",
      "SubGD iter. 134/499: loss=5.407322445682747, w0=72.17623762376245, w1=15.082070635568638\n",
      "SubGD iter. 135/499: loss=5.387252260502595, w0=72.24554455445552, w1=15.178225089360508\n",
      "SubGD iter. 136/499: loss=5.370460780338691, w0=72.30099009900998, w1=15.25972348971595\n",
      "SubGD iter. 137/499: loss=5.3574065233347365, w0=72.34950495049513, w1=15.335091856448178\n",
      "SubGD iter. 138/499: loss=5.345929264022579, w0=72.39801980198028, w1=15.410460223180406\n",
      "SubGD iter. 139/499: loss=5.335714659517469, w0=72.43267326732682, w1=15.469961786755766\n",
      "SubGD iter. 140/499: loss=5.330043910465359, w0=72.46039603960405, w1=15.51864528583285\n",
      "SubGD iter. 141/499: loss=5.325676428273224, w0=72.48811881188128, w1=15.561592159086528\n",
      "SubGD iter. 142/499: loss=5.322176726526588, w0=72.5019801980199, w1=15.597828332032567\n",
      "SubGD iter. 143/499: loss=5.32011130964311, w0=72.52277227722782, w1=15.624722856626754\n",
      "SubGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098041\n",
      "SubGD iter. 145/499: loss=5.317240048565146, w0=72.56435643564366, w1=15.664356578291132\n",
      "SubGD iter. 146/499: loss=5.316406547951545, w0=72.58514851485158, w1=15.677095775361325\n",
      "SubGD iter. 147/499: loss=5.315557122666141, w0=72.6059405940595, w1=15.689834972431518\n",
      "SubGD iter. 148/499: loss=5.31470769738074, w0=72.62673267326743, w1=15.70257416950171\n",
      "SubGD iter. 149/499: loss=5.313876880922164, w0=72.64059405940604, w1=15.724240418694801\n",
      "SubGD iter. 150/499: loss=5.313052246871382, w0=72.66138613861396, w1=15.736979615764994\n",
      "SubGD iter. 151/499: loss=5.3123778390243865, w0=72.66831683168327, w1=15.74811029423132\n",
      "SubGD iter. 152/499: loss=5.312132229725042, w0=72.67524752475258, w1=15.759240972697647\n",
      "SubGD iter. 153/499: loss=5.311886620425695, w0=72.68217821782189, w1=15.770371651163973\n",
      "SubGD iter. 154/499: loss=5.311683566098434, w0=72.68217821782189, w1=15.774323911906727\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264948\n",
      "SubGD iter. 156/499: loss=5.311638936484209, w0=72.68217821782189, w1=15.782228433392234\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134988\n",
      "SubGD iter. 158/499: loss=5.311594306869984, w0=72.68217821782189, w1=15.790132954877741\n",
      "SubGD iter. 159/499: loss=5.311571992062872, w0=72.68217821782189, w1=15.794085215620495\n",
      "SubGD iter. 160/499: loss=5.311549677255758, w0=72.68217821782189, w1=15.798037476363248\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737106002\n",
      "SubGD iter. 162/499: loss=5.311505047641535, w0=72.68217821782189, w1=15.805941997848755\n",
      "SubGD iter. 163/499: loss=5.3114827328344205, w0=72.68217821782189, w1=15.809894258591509\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334263\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780077016\n",
      "SubGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.82175104081977\n",
      "SubGD iter. 167/499: loss=5.311393473605971, w0=72.68217821782189, w1=15.825703301562523\n",
      "SubGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305277\n",
      "SubGD iter. 169/499: loss=5.311348843991747, w0=72.68217821782189, w1=15.83360782304803\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790784\n",
      "SubGD iter. 171/499: loss=5.311304214377522, w0=72.68217821782189, w1=15.841512344533538\n",
      "SubGD iter. 172/499: loss=5.31128189957041, w0=72.68217821782189, w1=15.845464605276291\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019045\n",
      "SubGD iter. 174/499: loss=5.311237269956185, w0=72.68217821782189, w1=15.853369126761798\n",
      "SubGD iter. 175/499: loss=5.311214955149072, w0=72.68217821782189, w1=15.857321387504552\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247305\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990059\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732812\n",
      "SubGD iter. 179/499: loss=5.311125695920622, w0=72.68217821782189, w1=15.873130430475566\n",
      "SubGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.87708269121832\n",
      "SubGD iter. 181/499: loss=5.311081066306398, w0=72.68217821782189, w1=15.881034951961073\n",
      "SubGD iter. 182/499: loss=5.311058751499286, w0=72.68217821782189, w1=15.884987212703827\n",
      "SubGD iter. 183/499: loss=5.311036436692172, w0=72.68217821782189, w1=15.88893947344658\n",
      "SubGD iter. 184/499: loss=5.31101412188506, w0=72.68217821782189, w1=15.892891734189334\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932087\n",
      "SubGD iter. 186/499: loss=5.310969492270836, w0=72.68217821782189, w1=15.900796255674841\n",
      "SubGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417595\n",
      "SubGD iter. 188/499: loss=5.310924862656612, w0=72.68217821782189, w1=15.908700777160348\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.912653037903102\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117364\n",
      "SubGD iter. 191/499: loss=5.310892237186269, w0=72.67524752475258, w1=15.914479198860118\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.918431459602871\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817134\n",
      "SubGD iter. 194/499: loss=5.310859611715928, w0=72.66831683168327, w1=15.920257620559887\n",
      "SubGD iter. 195/499: loss=5.310837296908815, w0=72.66831683168327, w1=15.924209881302641\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045394\n",
      "SubGD iter. 197/499: loss=5.310823570190172, w0=72.66138613861396, w1=15.926036042259657\n",
      "SubGD iter. 198/499: loss=5.310804671438475, w0=72.66138613861396, w1=15.92998830300241\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745164\n",
      "SubGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959427\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470218\n",
      "SubGD iter. 202/499: loss=5.310749731161019, w0=72.65445544554466, w1=15.939718985444934\n",
      "SubGD iter. 203/499: loss=5.310727416353907, w0=72.65445544554466, w1=15.943671246187687\n",
      "SubGD iter. 204/499: loss=5.310733434318959, w0=72.64752475247535, w1=15.94154514640195\n",
      "SubGD iter. 205/499: loss=5.310717105690678, w0=72.64752475247535, w1=15.945497407144703\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887457\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.94732356810172\n",
      "SubGD iter. 208/499: loss=5.310684480220336, w0=72.64059405940604, w1=15.951275828844473\n",
      "SubGD iter. 209/499: loss=5.310662165413224, w0=72.64059405940604, w1=15.955228089587226\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032998\n",
      "SubGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544243\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286996\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.96495877202975\n",
      "SubGD iter. 214/499: loss=5.3105922284402, w0=72.62673267326743, w1=15.962832672244012\n",
      "SubGD iter. 215/499: loss=5.310633183099026, w0=72.63366336633673, w1=15.967301372051416\n",
      "SubGD iter. 216/499: loss=5.3105993435850625, w0=72.62673267326743, w1=15.965175272265679\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073083\n",
      "SubGD iter. 218/499: loss=5.310606458729926, w0=72.62673267326743, w1=15.967517872287345\n",
      "SubGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.97198657209475\n",
      "SubGD iter. 220/499: loss=5.3106135738747895, w0=72.62673267326743, w1=15.969860472309012\n",
      "SubGD iter. 221/499: loss=5.310588318629316, w0=72.63366336633673, w1=15.974329172116416\n",
      "SubGD iter. 222/499: loss=5.310620689019653, w0=72.62673267326743, w1=15.972203072330679\n",
      "SubGD iter. 223/499: loss=5.310574966149885, w0=72.62673267326743, w1=15.970593411609592\n",
      "SubGD iter. 224/499: loss=5.31058363964973, w0=72.63366336633673, w1=15.975062111416996\n",
      "SubGD iter. 225/499: loss=5.310622915165494, w0=72.62673267326743, w1=15.972936011631258\n",
      "SubGD iter. 226/499: loss=5.310576651555034, w0=72.62673267326743, w1=15.971326350910171\n",
      "SubGD iter. 227/499: loss=5.310578960670142, w0=72.63366336633673, w1=15.975795050717576\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931838\n",
      "SubGD iter. 229/499: loss=5.31057833696018, w0=72.62673267326743, w1=15.972059290210751\n",
      "SubGD iter. 230/499: loss=5.310574635520698, w0=72.62673267326743, w1=15.970449629489664\n",
      "SubGD iter. 231/499: loss=5.310584557534202, w0=72.63366336633673, w1=15.974918329297068\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.972792229511331\n",
      "SubGD iter. 233/499: loss=5.310576320925845, w0=72.62673267326743, w1=15.971182568790244\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597648\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881191\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090824\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898228\n",
      "SubGD iter. 238/499: loss=5.310626930749844, w0=72.62673267326743, w1=15.97425810811249\n",
      "SubGD iter. 239/499: loss=5.310579691736141, w0=72.62673267326743, w1=15.972648447391403\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670317\n",
      "SubGD iter. 241/499: loss=5.310580796439089, w0=72.63366336633673, w1=15.97550748647772\n",
      "SubGD iter. 242/499: loss=5.310624267896666, w0=72.62673267326743, w1=15.973381386691983\n",
      "SubGD iter. 243/499: loss=5.310577675701806, w0=72.62673267326743, w1=15.971771725970896\n",
      "SubGD iter. 244/499: loss=5.3105761174595, w0=72.63366336633673, w1=15.9762404257783\n",
      "SubGD iter. 245/499: loss=5.310626494042512, w0=72.62673267326743, w1=15.974114325992563\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271476\n",
      "SubGD iter. 247/499: loss=5.310575659667473, w0=72.62673267326743, w1=15.970895004550389\n",
      "SubGD iter. 248/499: loss=5.310581714323562, w0=72.63366336633673, w1=15.975363704357793\n",
      "SubGD iter. 249/499: loss=5.310623831189333, w0=72.62673267326743, w1=15.973237604572056\n",
      "SubGD iter. 250/499: loss=5.310577345072619, w0=72.62673267326743, w1=15.971627943850969\n",
      "SubGD iter. 251/499: loss=5.310577035343974, w0=72.63366336633673, w1=15.976096643658373\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872635\n",
      "SubGD iter. 253/499: loss=5.310579030477768, w0=72.62673267326743, w1=15.972360883151548\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.970751222430462\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237866\n",
      "SubGD iter. 256/499: loss=5.310623394482, w0=72.62673267326743, w1=15.973093822452128\n",
      "SubGD iter. 257/499: loss=5.310577014443433, w0=72.62673267326743, w1=15.971484161731041\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538445\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752708\n",
      "SubGD iter. 260/499: loss=5.310578699848579, w0=72.62673267326743, w1=15.972217101031621\n",
      "SubGD iter. 261/499: loss=5.310574998409098, w0=72.62673267326743, w1=15.970607440310534\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117938\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.9729500403322\n",
      "SubGD iter. 264/499: loss=5.310576683814246, w0=72.62673267326743, w1=15.971340379611114\n",
      "SubGD iter. 265/499: loss=5.310578871112923, w0=72.63366336633673, w1=15.975809079418518\n",
      "SubGD iter. 266/499: loss=5.310625183920507, w0=72.62673267326743, w1=15.97368297963278\n",
      "SubGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911693\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190607\n",
      "SubGD iter. 269/499: loss=5.310584467976983, w0=72.63366336633673, w1=15.97493235799801\n",
      "SubGD iter. 270/499: loss=5.310622521067329, w0=72.62673267326743, w1=15.972806258212273\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491186\n",
      "SubGD iter. 272/499: loss=5.310579788997396, w0=72.63366336633673, w1=15.97566529729859\n",
      "SubGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512853\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791766\n",
      "SubGD iter. 275/499: loss=5.310575110017808, w0=72.63366336633673, w1=15.97639823659917\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813433\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092346\n",
      "SubGD iter. 278/499: loss=5.310576022555872, w0=72.62673267326743, w1=15.971052815371259\n",
      "SubGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178663\n",
      "SubGD iter. 280/499: loss=5.310624310505836, w0=72.62673267326743, w1=15.973395415392925\n",
      "SubGD iter. 281/499: loss=5.310577707961019, w0=72.62673267326743, w1=15.971785754671838\n",
      "SubGD iter. 282/499: loss=5.3105760279022824, w0=72.63366336633673, w1=15.976254454479243\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693505\n",
      "SubGD iter. 284/499: loss=5.310579393366167, w0=72.62673267326743, w1=15.972518693972418\n",
      "SubGD iter. 285/499: loss=5.310575691926685, w0=72.62673267326743, w1=15.970909033251331\n",
      "SubGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058735\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272998\n",
      "SubGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.971641972551911\n",
      "SubGD iter. 289/499: loss=5.310576945786756, w0=72.63366336633673, w1=15.976110672359315\n",
      "SubGD iter. 290/499: loss=5.310626099944345, w0=72.62673267326743, w1=15.973984572573578\n",
      "SubGD iter. 291/499: loss=5.310579062736981, w0=72.62673267326743, w1=15.97237491185249\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131404\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938808\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115307\n",
      "SubGD iter. 295/499: loss=5.310577046702646, w0=72.62673267326743, w1=15.971498190431983\n",
      "SubGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239388\n",
      "SubGD iter. 297/499: loss=5.310625663237009, w0=72.62673267326743, w1=15.97384079045365\n",
      "SubGD iter. 298/499: loss=5.310578732107793, w0=72.62673267326743, w1=15.972231129732563\n",
      "SubGD iter. 299/499: loss=5.310575030668311, w0=72.62673267326743, w1=15.970621469011476\n",
      "SubGD iter. 300/499: loss=5.31058346053529, w0=72.63366336633673, w1=15.97509016881888\n",
      "SubGD iter. 301/499: loss=5.310623000383833, w0=72.62673267326743, w1=15.972964069033143\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312056\n",
      "SubGD iter. 303/499: loss=5.310578781555704, w0=72.63366336633673, w1=15.97582310811946\n",
      "SubGD iter. 304/499: loss=5.310625226529675, w0=72.62673267326743, w1=15.973697008333723\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612636\n",
      "SubGD iter. 306/499: loss=5.310574700039124, w0=72.62673267326743, w1=15.970477686891549\n",
      "SubGD iter. 307/499: loss=5.310584378419764, w0=72.63366336633673, w1=15.974946386698953\n",
      "SubGD iter. 308/499: loss=5.310622563676497, w0=72.62673267326743, w1=15.972820286913215\n",
      "SubGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192129\n",
      "SubGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999533\n",
      "SubGD iter. 311/499: loss=5.310624789822341, w0=72.62673267326743, w1=15.973553226213795\n",
      "SubGD iter. 312/499: loss=5.310578070849419, w0=72.62673267326743, w1=15.971943565492708\n",
      "SubGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300112\n",
      "SubGD iter. 314/499: loss=5.310627015968183, w0=72.62673267326743, w1=15.974286165514375\n",
      "SubGD iter. 315/499: loss=5.310579756254565, w0=72.62673267326743, w1=15.972676504793288\n",
      "SubGD iter. 316/499: loss=5.310576054815084, w0=72.62673267326743, w1=15.971066844072201\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879605\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093868\n",
      "SubGD iter. 319/499: loss=5.310577740220233, w0=72.62673267326743, w1=15.97179978337278\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180185\n",
      "SubGD iter. 321/499: loss=5.310626579260847, w0=72.62673267326743, w1=15.974142383394447\n",
      "SubGD iter. 322/499: loss=5.31057942562538, w0=72.62673267326743, w1=15.97253272267336\n",
      "SubGD iter. 323/499: loss=5.310575724185898, w0=72.62673267326743, w1=15.970923061952274\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759678\n",
      "SubGD iter. 325/499: loss=5.310623916407669, w0=72.62673267326743, w1=15.97326566197394\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252853\n",
      "SubGD iter. 327/499: loss=5.310576856229536, w0=72.63366336633673, w1=15.976124701060257\n",
      "SubGD iter. 328/499: loss=5.310626142553513, w0=72.62673267326743, w1=15.97399860127452\n",
      "SubGD iter. 329/499: loss=5.310579094996192, w0=72.62673267326743, w1=15.972388940553433\n",
      "SubGD iter. 330/499: loss=5.31057539355671, w0=72.62673267326743, w1=15.970779279832346\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963975\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879854013\n",
      "SubGD iter. 333/499: loss=5.310577078961858, w0=72.62673267326743, w1=15.971512219132926\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.97598091894033\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154592\n",
      "SubGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433505\n",
      "SubGD iter. 337/499: loss=5.310575062927524, w0=72.62673267326743, w1=15.970635497712419\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519823\n",
      "SubGD iter. 339/499: loss=5.310623042993, w0=72.62673267326743, w1=15.972978097734085\n",
      "SubGD iter. 340/499: loss=5.310576748332672, w0=72.62673267326743, w1=15.971368437012998\n",
      "SubGD iter. 341/499: loss=5.310578691998485, w0=72.63366336633673, w1=15.975837136820402\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034665\n",
      "SubGD iter. 343/499: loss=5.310578433737819, w0=72.62673267326743, w1=15.972101376313578\n",
      "SubGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.970491715592491\n",
      "SubGD iter. 345/499: loss=5.310584288862545, w0=72.63366336633673, w1=15.974960415399895\n",
      "SubGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614158\n",
      "SubGD iter. 347/499: loss=5.3105764177034835, w0=72.62673267326743, w1=15.97122465489307\n",
      "SubGD iter. 348/499: loss=5.310579609882959, w0=72.63366336633673, w1=15.975693354700475\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914737\n",
      "SubGD iter. 350/499: loss=5.3105781031086305, w0=72.62673267326743, w1=15.97195759419365\n",
      "SubGD iter. 351/499: loss=5.310574930903369, w0=72.63366336633673, w1=15.976426294001055\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215317\n",
      "SubGD iter. 353/499: loss=5.310579788513779, w0=72.62673267326743, w1=15.97269053349423\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773143\n",
      "SubGD iter. 355/499: loss=5.310580527767431, w0=72.63366336633673, w1=15.975549572580547\n",
      "SubGD iter. 356/499: loss=5.310624395724175, w0=72.62673267326743, w1=15.97342347279481\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073723\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881127\n",
      "SubGD iter. 359/499: loss=5.310626621870016, w0=72.62673267326743, w1=15.97415641209539\n",
      "SubGD iter. 360/499: loss=5.310579457884592, w0=72.62673267326743, w1=15.972546751374303\n",
      "SubGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653216\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.97540579046062\n",
      "SubGD iter. 363/499: loss=5.310623959016838, w0=72.62673267326743, w1=15.973279690674882\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953795\n",
      "SubGD iter. 365/499: loss=5.310576766672318, w0=72.63366336633673, w1=15.9761387297612\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975462\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254375\n",
      "SubGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533288\n",
      "SubGD iter. 369/499: loss=5.310582363536379, w0=72.63366336633673, w1=15.975262008340692\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554955\n",
      "SubGD iter. 371/499: loss=5.310577111221071, w0=72.62673267326743, w1=15.971526247833868\n",
      "SubGD iter. 372/499: loss=5.310577684556792, w0=72.63366336633673, w1=15.975994947641272\n",
      "SubGD iter. 373/499: loss=5.310625748455347, w0=72.62673267326743, w1=15.973868847855535\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134448\n",
      "SubGD iter. 375/499: loss=5.310575095186737, w0=72.62673267326743, w1=15.97064952641336\n",
      "SubGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220765\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126435027\n",
      "SubGD iter. 378/499: loss=5.310576780591883, w0=72.62673267326743, w1=15.97138246571394\n",
      "SubGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521345\n",
      "SubGD iter. 380/499: loss=5.310625311748013, w0=72.62673267326743, w1=15.973725065735607\n",
      "SubGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501452\n",
      "SubGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293433\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100837\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.9728483443151\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683594013\n",
      "SubGD iter. 386/499: loss=5.310579520325739, w0=72.63366336633673, w1=15.975707383401417\n",
      "SubGD iter. 387/499: loss=5.310624875040677, w0=72.62673267326743, w1=15.97358128361568\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894593\n",
      "SubGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701997\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.97431422291626\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195172\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474086\n",
      "SubGD iter. 393/499: loss=5.310580438210212, w0=72.63366336633673, w1=15.97556360128149\n",
      "SubGD iter. 394/499: loss=5.310624438333342, w0=72.62673267326743, w1=15.973437501495752\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774665\n",
      "SubGD iter. 396/499: loss=5.310575759230625, w0=72.63366336633673, w1=15.97629654058207\n",
      "SubGD iter. 397/499: loss=5.3106266644791855, w0=72.62673267326743, w1=15.974170440796332\n",
      "SubGD iter. 398/499: loss=5.310579490143805, w0=72.62673267326743, w1=15.972560780075245\n",
      "SubGD iter. 399/499: loss=5.310575788704324, w0=72.62673267326743, w1=15.970951119354158\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161562\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375825\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654738\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462142\n",
      "SubGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676404\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955317\n",
      "SubGD iter. 406/499: loss=5.310575458075137, w0=72.62673267326743, w1=15.97080733723423\n",
      "SubGD iter. 407/499: loss=5.31058227397916, w0=72.63366336633673, w1=15.975276037041635\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255897\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653481\n",
      "SubGD iter. 410/499: loss=5.310577594999573, w0=72.63366336633673, w1=15.976008976342214\n",
      "SubGD iter. 411/499: loss=5.310625791064515, w0=72.62673267326743, w1=15.973882876556477\n",
      "SubGD iter. 412/499: loss=5.310578828885431, w0=72.62673267326743, w1=15.97227321583539\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114303\n",
      "SubGD iter. 414/499: loss=5.310583191863635, w0=72.63366336633673, w1=15.975132254921707\n",
      "SubGD iter. 415/499: loss=5.310623128211339, w0=72.62673267326743, w1=15.97300615513597\n",
      "SubGD iter. 416/499: loss=5.310576812851096, w0=72.62673267326743, w1=15.971396494414883\n",
      "SubGD iter. 417/499: loss=5.310578512884047, w0=72.63366336633673, w1=15.975865194222287\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.97373909443655\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715462\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994376\n",
      "SubGD iter. 421/499: loss=5.310584109748109, w0=72.63366336633673, w1=15.97498847280178\n",
      "SubGD iter. 422/499: loss=5.310622691504003, w0=72.62673267326743, w1=15.972862373016042\n",
      "SubGD iter. 423/499: loss=5.31057648222191, w0=72.62673267326743, w1=15.971252712294955\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.97572141210236\n",
      "SubGD iter. 425/499: loss=5.310624917649846, w0=72.62673267326743, w1=15.973595312316622\n",
      "SubGD iter. 426/499: loss=5.310578167627058, w0=72.62673267326743, w1=15.971985651595535\n",
      "SubGD iter. 427/499: loss=5.310574751788933, w0=72.63366336633673, w1=15.97645435140294\n",
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.974328251617202\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896115\n",
      "SubGD iter. 430/499: loss=5.3105761515927234, w0=72.62673267326743, w1=15.971108930175028\n",
      "SubGD iter. 431/499: loss=5.310580348652993, w0=72.63366336633673, w1=15.975577629982432\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196694\n",
      "SubGD iter. 433/499: loss=5.310577836997869, w0=72.62673267326743, w1=15.971841869475607\n",
      "SubGD iter. 434/499: loss=5.310575669673406, w0=72.63366336633673, w1=15.976310569283012\n",
      "SubGD iter. 435/499: loss=5.310626707088355, w0=72.62673267326743, w1=15.974184469497274\n",
      "SubGD iter. 436/499: loss=5.310579522403018, w0=72.62673267326743, w1=15.972574808776187\n",
      "SubGD iter. 437/499: loss=5.310575820963535, w0=72.62673267326743, w1=15.9709651480551\n",
      "SubGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862504\n",
      "SubGD iter. 439/499: loss=5.310624044235176, w0=72.62673267326743, w1=15.973307748076767\n",
      "SubGD iter. 440/499: loss=5.310577506368682, w0=72.62673267326743, w1=15.97169808735568\n",
      "SubGD iter. 441/499: loss=5.310576587557881, w0=72.63366336633673, w1=15.976166787163084\n",
      "SubGD iter. 442/499: loss=5.310626270381018, w0=72.62673267326743, w1=15.974040687377347\n",
      "SubGD iter. 443/499: loss=5.310579191773829, w0=72.62673267326743, w1=15.97243102665626\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935173\n",
      "SubGD iter. 445/499: loss=5.310582184421942, w0=72.63366336633673, w1=15.975290065742577\n",
      "SubGD iter. 446/499: loss=5.310623607527842, w0=72.62673267326743, w1=15.97316396595684\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235752\n",
      "SubGD iter. 448/499: loss=5.310577505442354, w0=72.63366336633673, w1=15.976023005043157\n",
      "SubGD iter. 449/499: loss=5.310625833673685, w0=72.62673267326743, w1=15.97389690525742\n",
      "SubGD iter. 450/499: loss=5.310578861144642, w0=72.62673267326743, w1=15.972287244536332\n",
      "SubGD iter. 451/499: loss=5.310575159705164, w0=72.62673267326743, w1=15.970677583815245\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.97514628362265\n",
      "SubGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836912\n",
      "SubGD iter. 454/499: loss=5.31057684511031, w0=72.62673267326743, w1=15.971410523115825\n",
      "SubGD iter. 455/499: loss=5.310578423326829, w0=72.63366336633673, w1=15.97587922292323\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.973753123137492\n",
      "SubGD iter. 457/499: loss=5.310578530515457, w0=72.62673267326743, w1=15.972143462416405\n",
      "SubGD iter. 458/499: loss=5.310574829075975, w0=72.62673267326743, w1=15.970533801695318\n",
      "SubGD iter. 459/499: loss=5.31058402019089, w0=72.63366336633673, w1=15.975002501502722\n",
      "SubGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716984\n",
      "SubGD iter. 461/499: loss=5.310576514481121, w0=72.62673267326743, w1=15.971266740995897\n",
      "SubGD iter. 462/499: loss=5.310579341211301, w0=72.63366336633673, w1=15.975735440803302\n",
      "SubGD iter. 463/499: loss=5.310624960259015, w0=72.62673267326743, w1=15.973609341017564\n",
      "SubGD iter. 464/499: loss=5.310578199886269, w0=72.62673267326743, w1=15.971999680296477\n",
      "SubGD iter. 465/499: loss=5.310574662231715, w0=72.63366336633673, w1=15.976468380103881\n",
      "SubGD iter. 466/499: loss=5.310627186404856, w0=72.62673267326743, w1=15.974342280318144\n",
      "SubGD iter. 467/499: loss=5.310579885291418, w0=72.62673267326743, w1=15.972732619597057\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887597\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683374\n",
      "SubGD iter. 470/499: loss=5.31062452355168, w0=72.62673267326743, w1=15.973465558897637\n",
      "SubGD iter. 471/499: loss=5.310577869257082, w0=72.62673267326743, w1=15.97185589817655\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983954\n",
      "SubGD iter. 473/499: loss=5.310626749697523, w0=72.62673267326743, w1=15.974198498198216\n",
      "SubGD iter. 474/499: loss=5.310579554662228, w0=72.62673267326743, w1=15.97258883747713\n",
      "SubGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756043\n",
      "SubGD iter. 476/499: loss=5.310581176980249, w0=72.63366336633673, w1=15.975447876563447\n",
      "SubGD iter. 477/499: loss=5.310624086844345, w0=72.62673267326743, w1=15.97332177677771\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056622\n",
      "SubGD iter. 479/499: loss=5.310576498000661, w0=72.63366336633673, w1=15.976180815864026\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078289\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357202\n",
      "SubGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636115\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.97530409444352\n",
      "SubGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.973177994657782\n",
      "SubGD iter. 485/499: loss=5.3105772079987075, w0=72.62673267326743, w1=15.971568333936695\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744099\n",
      "SubGD iter. 487/499: loss=5.310625876282853, w0=72.62673267326743, w1=15.973910933958361\n",
      "SubGD iter. 488/499: loss=5.310578893403855, w0=72.62673267326743, w1=15.972301273237274\n",
      "SubGD iter. 489/499: loss=5.310575191964375, w0=72.62673267326743, w1=15.970691612516188\n",
      "SubGD iter. 490/499: loss=5.310583012749197, w0=72.63366336633673, w1=15.975160312323592\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537854\n",
      "SubGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816767\n",
      "SubGD iter. 493/499: loss=5.310578333769609, w0=72.63366336633673, w1=15.975893251624171\n",
      "SubGD iter. 494/499: loss=5.310625439575519, w0=72.62673267326743, w1=15.973767151838434\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117347\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039626\n",
      "SubGD iter. 497/499: loss=5.310583930633671, w0=72.63366336633673, w1=15.975016530203664\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417927\n",
      "SubGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.97128076969684\n",
      "SubGD: execution time=0.006 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186962e35e2a450882d85babbde8cc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Compute a stochastic subgradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    batches = batch_iter(y, tx, batch_size, num_batches=max_iters)\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        batch_y, batch_tx = next(batches)\n",
    "        gradient = compute_subgradient_mae(batch_y, batch_tx, w)\n",
    "        w = w - gamma * gradient\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=73.29392200210518, w0=0.7, w1=0.6697557600892412\n",
      "SubSGD iter. 1/499: loss=72.59392200210517, w0=1.4, w1=0.660535967828516\n",
      "SubSGD iter. 2/499: loss=71.89392200210517, w0=2.0999999999999996, w1=1.0330310682823483\n",
      "SubSGD iter. 3/499: loss=71.19392200210518, w0=2.8, w1=1.3637671468399666\n",
      "SubSGD iter. 4/499: loss=70.49392200210517, w0=3.5, w1=2.2828558760024267\n",
      "SubSGD iter. 5/499: loss=69.79392200210518, w0=4.2, w1=0.5314950648931573\n",
      "SubSGD iter. 6/499: loss=69.09392200210517, w0=4.9, w1=0.8353629281300136\n",
      "SubSGD iter. 7/499: loss=68.39392200210517, w0=5.6000000000000005, w1=1.1449157538984478\n",
      "SubSGD iter. 8/499: loss=67.69392200210517, w0=6.300000000000001, w1=0.012623254088416358\n",
      "SubSGD iter. 9/499: loss=66.99392200210517, w0=7.000000000000001, w1=-0.05334260546429688\n",
      "SubSGD iter. 10/499: loss=66.29392200210518, w0=7.700000000000001, w1=0.43858175160096124\n",
      "SubSGD iter. 11/499: loss=65.59392200210519, w0=8.4, w1=0.8032384522572216\n",
      "SubSGD iter. 12/499: loss=64.89392200210519, w0=9.1, w1=-0.025730029715825276\n",
      "SubSGD iter. 13/499: loss=64.19392200210518, w0=9.799999999999999, w1=-0.22568703298095966\n",
      "SubSGD iter. 14/499: loss=63.49392200210518, w0=10.499999999999998, w1=-0.8786922599747817\n",
      "SubSGD iter. 15/499: loss=62.793922002105184, w0=11.199999999999998, w1=-0.5058510684835349\n",
      "SubSGD iter. 16/499: loss=62.09392200210518, w0=11.899999999999997, w1=-0.31299852507690473\n",
      "SubSGD iter. 17/499: loss=61.393922002105185, w0=12.599999999999996, w1=0.7287966619964383\n",
      "SubSGD iter. 18/499: loss=60.69392200210518, w0=13.299999999999995, w1=0.6860480813451678\n",
      "SubSGD iter. 19/499: loss=59.99392200210517, w0=13.999999999999995, w1=-0.24582075373007461\n",
      "SubSGD iter. 20/499: loss=59.293922002105184, w0=14.699999999999994, w1=-0.8621343102463989\n",
      "SubSGD iter. 21/499: loss=58.593922002105174, w0=15.399999999999993, w1=-1.5151415635031165\n",
      "SubSGD iter. 22/499: loss=57.893922002105185, w0=16.099999999999994, w1=-1.35834532772304\n",
      "SubSGD iter. 23/499: loss=57.19392200210517, w0=16.799999999999994, w1=-1.3408996401550386\n",
      "SubSGD iter. 24/499: loss=56.49392200210518, w0=17.499999999999993, w1=-0.8685773865323012\n",
      "SubSGD iter. 25/499: loss=55.793922002105184, w0=18.199999999999992, w1=-0.41686482221728877\n",
      "SubSGD iter. 26/499: loss=55.09392200210518, w0=18.89999999999999, w1=-0.2734195533102542\n",
      "SubSGD iter. 27/499: loss=54.393922002105185, w0=19.59999999999999, w1=-0.45650424197400075\n",
      "SubSGD iter. 28/499: loss=53.69392200210518, w0=20.29999999999999, w1=0.7708209113347476\n",
      "SubSGD iter. 29/499: loss=52.99392200210518, w0=20.99999999999999, w1=1.7333233138059203\n",
      "SubSGD iter. 30/499: loss=52.293922002105184, w0=21.69999999999999, w1=2.1713403811196352\n",
      "SubSGD iter. 31/499: loss=51.59392200210519, w0=22.399999999999988, w1=3.186756999869518\n",
      "SubSGD iter. 32/499: loss=50.893922002105185, w0=23.099999999999987, w1=4.350800753061656\n",
      "SubSGD iter. 33/499: loss=50.19392200210519, w0=23.799999999999986, w1=3.6281998283496977\n",
      "SubSGD iter. 34/499: loss=49.493922002105194, w0=24.499999999999986, w1=4.0392708559432\n",
      "SubSGD iter. 35/499: loss=48.79392200210519, w0=25.199999999999985, w1=3.1266244380898343\n",
      "SubSGD iter. 36/499: loss=48.09392200210519, w0=25.899999999999984, w1=2.176081499237824\n",
      "SubSGD iter. 37/499: loss=47.39392200210519, w0=26.599999999999984, w1=3.537017801769098\n",
      "SubSGD iter. 38/499: loss=46.6939220021052, w0=27.299999999999983, w1=4.584099812880491\n",
      "SubSGD iter. 39/499: loss=45.993922002105194, w0=27.999999999999982, w1=4.2626120260979725\n",
      "SubSGD iter. 40/499: loss=45.29392200210519, w0=28.69999999999998, w1=3.4879299862735533\n",
      "SubSGD iter. 41/499: loss=44.5939220021052, w0=29.39999999999998, w1=4.459095274471158\n",
      "SubSGD iter. 42/499: loss=43.8939220021052, w0=30.09999999999998, w1=3.692015220059409\n",
      "SubSGD iter. 43/499: loss=43.1939220021052, w0=30.79999999999998, w1=4.037973629976709\n",
      "SubSGD iter. 44/499: loss=42.493922002105194, w0=31.49999999999998, w1=2.836678633395727\n",
      "SubSGD iter. 45/499: loss=41.7939220021052, w0=32.19999999999998, w1=3.686000134554671\n",
      "SubSGD iter. 46/499: loss=41.093922002105195, w0=32.899999999999984, w1=4.336267256324202\n",
      "SubSGD iter. 47/499: loss=40.39392200210519, w0=33.59999999999999, w1=4.08843146360168\n",
      "SubSGD iter. 48/499: loss=39.6939220021052, w0=34.29999999999999, w1=3.9027044202923076\n",
      "SubSGD iter. 49/499: loss=38.993922002105194, w0=34.99999999999999, w1=4.149533818880781\n",
      "SubSGD iter. 50/499: loss=38.29392200210519, w0=35.699999999999996, w1=5.03272749633209\n",
      "SubSGD iter. 51/499: loss=37.59392200210518, w0=36.4, w1=5.497749804244263\n",
      "SubSGD iter. 52/499: loss=36.893922002105185, w0=37.1, w1=6.185946158852271\n",
      "SubSGD iter. 53/499: loss=36.19392200210519, w0=37.800000000000004, w1=4.865339143808502\n",
      "SubSGD iter. 54/499: loss=35.49392200210518, w0=38.50000000000001, w1=4.162187112085409\n",
      "SubSGD iter. 55/499: loss=34.79392200210517, w0=39.20000000000001, w1=4.567153860941846\n",
      "SubSGD iter. 56/499: loss=34.093922002105174, w0=39.90000000000001, w1=4.6201366321608965\n",
      "SubSGD iter. 57/499: loss=33.39392200210517, w0=40.600000000000016, w1=4.385503084733239\n",
      "SubSGD iter. 58/499: loss=32.69392200210516, w0=41.30000000000002, w1=4.857525705724345\n",
      "SubSGD iter. 59/499: loss=31.99392200210516, w0=42.00000000000002, w1=4.5048201616229715\n",
      "SubSGD iter. 60/499: loss=31.293922002105152, w0=42.700000000000024, w1=5.012331798641759\n",
      "SubSGD iter. 61/499: loss=30.593922002105153, w0=43.40000000000003, w1=5.148704654525659\n",
      "SubSGD iter. 62/499: loss=29.893922002105153, w0=44.10000000000003, w1=4.206687145066815\n",
      "SubSGD iter. 63/499: loss=29.19471739093818, w0=44.80000000000003, w1=5.292150348152942\n",
      "SubSGD iter. 64/499: loss=28.493922002105148, w0=45.500000000000036, w1=5.283055512099128\n",
      "SubSGD iter. 65/499: loss=27.793940605389466, w0=46.20000000000004, w1=5.680267607058088\n",
      "SubSGD iter. 66/499: loss=27.093922002105142, w0=46.90000000000004, w1=4.906025244574746\n",
      "SubSGD iter. 67/499: loss=26.395924384491146, w0=47.600000000000044, w1=3.493243593497287\n",
      "SubSGD iter. 68/499: loss=25.70633960291817, w0=48.30000000000005, w1=3.381393423987567\n",
      "SubSGD iter. 69/499: loss=25.011828889320487, w0=49.00000000000005, w1=3.2673895088958913\n",
      "SubSGD iter. 70/499: loss=24.320999150004585, w0=49.70000000000005, w1=2.785821085878867\n",
      "SubSGD iter. 71/499: loss=23.65086055347355, w0=50.400000000000055, w1=3.0636891564965985\n",
      "SubSGD iter. 72/499: loss=22.955111523886423, w0=51.10000000000006, w1=3.317347127320808\n",
      "SubSGD iter. 73/499: loss=22.261500767426888, w0=51.80000000000006, w1=3.02903475391196\n",
      "SubSGD iter. 74/499: loss=21.606755413056245, w0=52.500000000000064, w1=1.8700956233691455\n",
      "SubSGD iter. 75/499: loss=21.079691635263227, w0=53.20000000000007, w1=1.6366645536776516\n",
      "SubSGD iter. 76/499: loss=20.485922774174508, w0=53.90000000000007, w1=0.6418965997997216\n",
      "SubSGD iter. 77/499: loss=20.098175717913218, w0=54.60000000000007, w1=1.0227096903015633\n",
      "SubSGD iter. 78/499: loss=19.414811042063974, w0=55.300000000000075, w1=1.938599132526889\n",
      "SubSGD iter. 79/499: loss=18.595594133497077, w0=56.00000000000008, w1=1.7444418800250436\n",
      "SubSGD iter. 80/499: loss=18.066088553011447, w0=56.70000000000008, w1=1.5053351071688559\n",
      "SubSGD iter. 81/499: loss=17.581300146300034, w0=57.400000000000084, w1=2.4667212249776806\n",
      "SubSGD iter. 82/499: loss=16.73148726135782, w0=58.10000000000009, w1=2.5352784342709156\n",
      "SubSGD iter. 83/499: loss=16.164854090343912, w0=58.80000000000009, w1=2.5605401577639566\n",
      "SubSGD iter. 84/499: loss=15.629973376412748, w0=59.50000000000009, w1=1.8098369768031235\n",
      "SubSGD iter. 85/499: loss=15.431824775868032, w0=60.200000000000095, w1=1.2032566188328149\n",
      "SubSGD iter. 86/499: loss=15.257520462070874, w0=59.50000000000009, w1=1.8809045889573026\n",
      "SubSGD iter. 87/499: loss=15.401686306758073, w0=60.200000000000095, w1=2.485365718044208\n",
      "SubSGD iter. 88/499: loss=14.674706546970734, w0=60.9000000000001, w1=2.296347243211221\n",
      "SubSGD iter. 89/499: loss=14.304376252192293, w0=61.6000000000001, w1=2.8396080811845446\n",
      "SubSGD iter. 90/499: loss=13.615591290347544, w0=62.300000000000104, w1=3.936170674311799\n",
      "SubSGD iter. 91/499: loss=12.67867866176235, w0=63.00000000000011, w1=3.502988859851188\n",
      "SubSGD iter. 92/499: loss=12.46254354178023, w0=63.70000000000011, w1=3.749328860434995\n",
      "SubSGD iter. 93/499: loss=11.950310883719865, w0=64.4000000000001, w1=3.9461816875638736\n",
      "SubSGD iter. 94/499: loss=11.481839658205867, w0=65.10000000000011, w1=4.397844947398922\n",
      "SubSGD iter. 95/499: loss=10.892680641215613, w0=64.4000000000001, w1=5.206233322992206\n",
      "SubSGD iter. 96/499: loss=10.838584939081949, w0=65.10000000000011, w1=5.724059436327319\n",
      "SubSGD iter. 97/499: loss=10.205273617809398, w0=65.80000000000011, w1=6.125268806618909\n",
      "SubSGD iter. 98/499: loss=9.64316170759123, w0=66.50000000000011, w1=5.691801738630305\n",
      "SubSGD iter. 99/499: loss=9.540007544178504, w0=67.20000000000012, w1=6.032252906599934\n",
      "SubSGD iter. 100/499: loss=9.058378882874429, w0=66.50000000000011, w1=6.7446175046598915\n",
      "SubSGD iter. 101/499: loss=8.990088518476066, w0=67.20000000000012, w1=7.174467503681169\n",
      "SubSGD iter. 102/499: loss=8.448824065383802, w0=66.50000000000011, w1=7.592542935520736\n",
      "SubSGD iter. 103/499: loss=8.594623724783151, w0=67.20000000000012, w1=8.292080093762031\n",
      "SubSGD iter. 104/499: loss=7.928978675092505, w0=67.90000000000012, w1=8.45759278022704\n",
      "SubSGD iter. 105/499: loss=7.5166252823344974, w0=68.60000000000012, w1=8.589258240471583\n",
      "SubSGD iter. 106/499: loss=7.151456376065839, w0=69.30000000000013, w1=9.149433335957156\n",
      "SubSGD iter. 107/499: loss=6.61234749176132, w0=70.00000000000013, w1=8.801125048251334\n",
      "SubSGD iter. 108/499: loss=6.555450078063067, w0=70.70000000000013, w1=9.559220947798254\n",
      "SubSGD iter. 109/499: loss=5.976635467278565, w0=70.00000000000013, w1=10.692490140953888\n",
      "SubSGD iter. 110/499: loss=5.701711590561409, w0=70.70000000000013, w1=10.63102010226838\n",
      "SubSGD iter. 111/499: loss=5.494828731017504, w0=71.40000000000013, w1=11.374084363270914\n",
      "SubSGD iter. 112/499: loss=5.037497216277888, w0=70.70000000000013, w1=10.508066681721603\n",
      "SubSGD iter. 113/499: loss=5.544360990381962, w0=70.00000000000013, w1=9.629139100361275\n",
      "SubSGD iter. 114/499: loss=6.145370096785102, w0=70.70000000000013, w1=10.565449000118583\n",
      "SubSGD iter. 115/499: loss=5.521092259677681, w0=70.00000000000013, w1=11.67680599200464\n",
      "SubSGD iter. 116/499: loss=5.396854838267091, w0=70.70000000000013, w1=12.01490596485306\n",
      "SubSGD iter. 117/499: loss=5.053876575945222, w0=71.40000000000013, w1=12.754695771973044\n",
      "SubSGD iter. 118/499: loss=4.714278155800014, w0=72.10000000000014, w1=14.191020101114782\n",
      "SubSGD iter. 119/499: loss=4.546642523525271, w0=71.40000000000013, w1=14.336229414852134\n",
      "SubSGD iter. 120/499: loss=4.706431300276078, w0=72.10000000000014, w1=14.10812908811178\n",
      "SubSGD iter. 121/499: loss=4.539013770526581, w0=71.40000000000013, w1=14.493708055484964\n",
      "SubSGD iter. 122/499: loss=4.725993082360241, w0=72.10000000000014, w1=13.55553804881851\n",
      "SubSGD iter. 123/499: loss=4.515585762322505, w0=72.80000000000014, w1=14.139398833448205\n",
      "SubSGD iter. 124/499: loss=4.464639118046848, w0=72.10000000000014, w1=13.964861636268337\n",
      "SubSGD iter. 125/499: loss=4.528236477516095, w0=72.80000000000014, w1=13.39788395516743\n",
      "SubSGD iter. 126/499: loss=4.43985610143174, w0=72.10000000000014, w1=12.64033882977427\n",
      "SubSGD iter. 127/499: loss=4.584667599415225, w0=72.80000000000014, w1=13.068447723036225\n",
      "SubSGD iter. 128/499: loss=4.4567326155671125, w0=72.10000000000014, w1=14.025244534061816\n",
      "SubSGD iter. 129/499: loss=4.532397776818291, w0=72.80000000000014, w1=12.801079543991861\n",
      "SubSGD iter. 130/499: loss=4.484396247563219, w0=73.50000000000014, w1=12.80368115868479\n",
      "SubSGD iter. 131/499: loss=4.475038277601774, w0=74.20000000000014, w1=12.34490705581811\n",
      "SubSGD iter. 132/499: loss=4.608477681798529, w0=73.50000000000014, w1=12.785407723243985\n",
      "SubSGD iter. 133/499: loss=4.4774257245838776, w0=72.80000000000014, w1=12.441214738815257\n",
      "SubSGD iter. 134/499: loss=4.541058404434901, w0=73.50000000000014, w1=13.817862102821703\n",
      "SubSGD iter. 135/499: loss=4.436040781887824, w0=74.20000000000014, w1=13.20426009268546\n",
      "SubSGD iter. 136/499: loss=4.502984136625173, w0=73.50000000000014, w1=11.838527262227034\n",
      "SubSGD iter. 137/499: loss=4.671625928601987, w0=72.80000000000014, w1=11.90524713214643\n",
      "SubSGD iter. 138/499: loss=4.665023553341964, w0=73.50000000000014, w1=12.305778787047005\n",
      "SubSGD iter. 139/499: loss=4.559304159240604, w0=72.80000000000014, w1=11.340049824923918\n",
      "SubSGD iter. 140/499: loss=4.839226006973578, w0=72.10000000000014, w1=12.344238651476076\n",
      "SubSGD iter. 141/499: loss=4.636647407899354, w0=71.40000000000013, w1=12.344540225541218\n",
      "SubSGD iter. 142/499: loss=4.779909168912712, w0=72.10000000000014, w1=12.869277556247088\n",
      "SubSGD iter. 143/499: loss=4.554952699294538, w0=72.80000000000014, w1=13.196512476088\n",
      "SubSGD iter. 144/499: loss=4.448092822559913, w0=73.50000000000014, w1=14.426219349473325\n",
      "SubSGD iter. 145/499: loss=4.49197316398844, w0=74.20000000000014, w1=14.640551313045215\n",
      "SubSGD iter. 146/499: loss=4.587954054357935, w0=74.90000000000015, w1=15.02695543139463\n",
      "SubSGD iter. 147/499: loss=4.788661318820507, w0=75.60000000000015, w1=15.591325450104387\n",
      "SubSGD iter. 148/499: loss=5.113876397099587, w0=76.30000000000015, w1=15.557924327194549\n",
      "SubSGD iter. 149/499: loss=5.349768264732505, w0=77.00000000000016, w1=15.103129821305236\n",
      "SubSGD iter. 150/499: loss=5.557637440769028, w0=76.30000000000015, w1=16.45303210182867\n",
      "SubSGD iter. 151/499: loss=5.617433394039216, w0=75.60000000000015, w1=15.005099700226166\n",
      "SubSGD iter. 152/499: loss=4.9740326626792335, w0=76.30000000000015, w1=14.977355683356793\n",
      "SubSGD iter. 153/499: loss=5.221413606468051, w0=75.60000000000015, w1=14.059124423234117\n",
      "SubSGD iter. 154/499: loss=4.8456353859767285, w0=76.30000000000015, w1=14.096097792532575\n",
      "SubSGD iter. 155/499: loss=5.108649827326679, w0=75.60000000000015, w1=13.460407098487925\n",
      "SubSGD iter. 156/499: loss=4.830095069507511, w0=74.90000000000015, w1=14.290812393716484\n",
      "SubSGD iter. 157/499: loss=4.668369936620652, w0=74.20000000000014, w1=14.637806286534499\n",
      "SubSGD iter. 158/499: loss=4.587503649257563, w0=74.90000000000015, w1=14.699324051410935\n",
      "SubSGD iter. 159/499: loss=4.726003918677512, w0=74.20000000000014, w1=13.990070258152228\n",
      "SubSGD iter. 160/499: loss=4.510684907086355, w0=73.50000000000014, w1=14.704164900069985\n",
      "SubSGD iter. 161/499: loss=4.534428760365148, w0=72.80000000000014, w1=14.144172476178968\n",
      "SubSGD iter. 162/499: loss=4.465086212766085, w0=72.10000000000014, w1=13.94838216183811\n",
      "SubSGD iter. 163/499: loss=4.527221108583848, w0=72.80000000000014, w1=14.177070873585329\n",
      "SubSGD iter. 164/499: loss=4.4682374612085, w0=72.10000000000014, w1=14.140836247796178\n",
      "SubSGD iter. 165/499: loss=4.541892768012197, w0=72.80000000000014, w1=14.345605557881617\n",
      "SubSGD iter. 166/499: loss=4.486874291931521, w0=72.10000000000014, w1=14.779922111670333\n",
      "SubSGD iter. 167/499: loss=4.628462558997665, w0=72.80000000000014, w1=13.934312754826097\n",
      "SubSGD iter. 168/499: loss=4.449049090632675, w0=72.10000000000014, w1=13.828850991956829\n",
      "SubSGD iter. 169/499: loss=4.521082766622293, w0=71.40000000000013, w1=14.876887854072546\n",
      "SubSGD iter. 170/499: loss=4.7882111531470235, w0=72.10000000000014, w1=14.233407648044496\n",
      "SubSGD iter. 171/499: loss=4.550954892249428, w0=72.80000000000014, w1=12.97985994385335\n",
      "SubSGD iter. 172/499: loss=4.4644103260121115, w0=73.50000000000014, w1=12.556851756223189\n",
      "SubSGD iter. 173/499: loss=4.512078935318996, w0=74.20000000000014, w1=12.794184120634569\n",
      "SubSGD iter. 174/499: loss=4.5387154256689595, w0=73.50000000000014, w1=12.178660460055976\n",
      "SubSGD iter. 175/499: loss=4.5863678377831985, w0=72.80000000000014, w1=12.168734133561452\n",
      "SubSGD iter. 176/499: loss=4.5984345381529375, w0=73.50000000000014, w1=12.905995968810506\n",
      "SubSGD iter. 177/499: loss=4.463078637809387, w0=74.20000000000014, w1=12.69078178551308\n",
      "SubSGD iter. 178/499: loss=4.551883648035866, w0=74.90000000000015, w1=11.133297271627127\n",
      "SubSGD iter. 179/499: loss=5.061796352176852, w0=75.60000000000015, w1=11.361077962893482\n",
      "SubSGD iter. 180/499: loss=5.157353946755696, w0=74.90000000000015, w1=11.374206193781461\n",
      "SubSGD iter. 181/499: loss=4.980904121994138, w0=74.20000000000014, w1=12.354911276588705\n",
      "SubSGD iter. 182/499: loss=4.606546324956566, w0=74.90000000000015, w1=12.283135287326983\n",
      "SubSGD iter. 183/499: loss=4.74824971634438, w0=74.20000000000014, w1=12.052417265741498\n",
      "SubSGD iter. 184/499: loss=4.6722485185567795, w0=74.90000000000015, w1=12.6007578586317\n",
      "SubSGD iter. 185/499: loss=4.696079331210986, w0=74.20000000000014, w1=13.173813731395716\n",
      "SubSGD iter. 186/499: loss=4.5046943887436, w0=74.90000000000015, w1=13.576377319864436\n",
      "SubSGD iter. 187/499: loss=4.628646268338439, w0=74.20000000000014, w1=13.44121329959043\n",
      "SubSGD iter. 188/499: loss=4.494985518760168, w0=73.50000000000014, w1=13.753816447153852\n",
      "SubSGD iter. 189/499: loss=4.433479929773189, w0=74.20000000000014, w1=14.75252012551331\n",
      "SubSGD iter. 190/499: loss=4.607046490283043, w0=73.50000000000014, w1=15.06430294683555\n",
      "SubSGD iter. 191/499: loss=4.605600543882679, w0=74.20000000000014, w1=14.283812278463511\n",
      "SubSGD iter. 192/499: loss=4.537952040975969, w0=73.50000000000014, w1=14.788847043919146\n",
      "SubSGD iter. 193/499: loss=4.549629264781953, w0=74.20000000000014, w1=13.967086385368138\n",
      "SubSGD iter. 194/499: loss=4.509106050767874, w0=74.90000000000015, w1=13.546295353631606\n",
      "SubSGD iter. 195/499: loss=4.628564154491482, w0=74.20000000000014, w1=13.582116121239855\n",
      "SubSGD iter. 196/499: loss=4.4942685010957435, w0=74.90000000000015, w1=13.134185639192602\n",
      "SubSGD iter. 197/499: loss=4.641856081631301, w0=74.20000000000014, w1=12.406803573921296\n",
      "SubSGD iter. 198/499: loss=4.596781898311346, w0=74.90000000000015, w1=13.13314818039439\n",
      "SubSGD iter. 199/499: loss=4.641924724312143, w0=74.20000000000014, w1=13.249406735598692\n",
      "SubSGD iter. 200/499: loss=4.500778831445927, w0=74.90000000000015, w1=11.646688642820191\n",
      "SubSGD iter. 201/499: loss=4.899164946953198, w0=74.20000000000014, w1=12.277676514945878\n",
      "SubSGD iter. 202/499: loss=4.621886637763542, w0=73.50000000000014, w1=12.35549975845566\n",
      "SubSGD iter. 203/499: loss=4.549266154367813, w0=72.80000000000014, w1=12.728709189288056\n",
      "SubSGD iter. 204/499: loss=4.494178855625835, w0=72.10000000000014, w1=11.523628737829815\n",
      "SubSGD iter. 205/499: loss=4.853302011794214, w0=72.80000000000014, w1=11.875741392309935\n",
      "SubSGD iter. 206/499: loss=4.673023740616479, w0=73.50000000000014, w1=12.916574455798745\n",
      "SubSGD iter. 207/499: loss=4.461964559542702, w0=74.20000000000014, w1=13.791545669297875\n",
      "SubSGD iter. 208/499: loss=4.499521539366134, w0=74.90000000000015, w1=14.756521729049066\n",
      "SubSGD iter. 209/499: loss=4.735901078498085, w0=74.20000000000014, w1=15.140296146146547\n",
      "SubSGD iter. 210/499: loss=4.686872703444019, w0=73.50000000000014, w1=14.300676305424153\n",
      "SubSGD iter. 211/499: loss=4.476195952821716, w0=74.20000000000014, w1=14.064795565961772\n",
      "SubSGD iter. 212/499: loss=4.516487039147157, w0=73.50000000000014, w1=13.563884568663296\n",
      "SubSGD iter. 213/499: loss=4.429518127531306, w0=72.80000000000014, w1=12.644814697603085\n",
      "SubSGD iter. 214/499: loss=4.506496161665907, w0=72.10000000000014, w1=13.191197044648238\n",
      "SubSGD iter. 215/499: loss=4.527067376243329, w0=72.80000000000014, w1=12.990528714261606\n",
      "SubSGD iter. 216/499: loss=4.463398529969405, w0=73.50000000000014, w1=13.874438970408242\n",
      "SubSGD iter. 217/499: loss=4.438848446990306, w0=72.80000000000014, w1=13.084332245694757\n",
      "SubSGD iter. 218/499: loss=4.45551450217038, w0=72.10000000000014, w1=13.187610496069171\n",
      "SubSGD iter. 219/499: loss=4.52728981098102, w0=71.40000000000013, w1=14.479642149805338\n",
      "SubSGD iter. 220/499: loss=4.7241057471970915, w0=72.10000000000014, w1=15.475626916128146\n",
      "SubSGD iter. 221/499: loss=4.7863271907067935, w0=72.80000000000014, w1=15.612591630840136\n",
      "SubSGD iter. 222/499: loss=4.752222727609549, w0=72.10000000000014, w1=15.418269278370776\n",
      "SubSGD iter. 223/499: loss=4.771036837993905, w0=71.40000000000013, w1=15.772812472249171\n",
      "SubSGD iter. 224/499: loss=5.005547965205228, w0=72.10000000000014, w1=16.748654797461292\n",
      "SubSGD iter. 225/499: loss=5.216071734365007, w0=72.80000000000014, w1=17.62751353921883\n",
      "SubSGD iter. 226/499: loss=5.542481013958723, w0=72.10000000000014, w1=16.948500893025784\n",
      "SubSGD iter. 227/499: loss=5.2981036962560575, w0=72.80000000000014, w1=17.37084686867928\n",
      "SubSGD iter. 228/499: loss=5.422481719407495, w0=72.10000000000014, w1=16.4449147427436\n",
      "SubSGD iter. 229/499: loss=5.098665091068152, w0=71.40000000000013, w1=16.484656388939985\n",
      "SubSGD iter. 230/499: loss=5.240422418150418, w0=72.10000000000014, w1=16.52043030131262\n",
      "SubSGD iter. 231/499: loss=5.127095487021414, w0=72.80000000000014, w1=15.416237364367088\n",
      "SubSGD iter. 232/499: loss=4.697798245868074, w0=72.10000000000014, w1=15.356605286218944\n",
      "SubSGD iter. 233/499: loss=4.754995514328509, w0=72.80000000000014, w1=15.479059117805473\n",
      "SubSGD iter. 234/499: loss=4.714748631048902, w0=73.50000000000014, w1=14.147583550443215\n",
      "SubSGD iter. 235/499: loss=4.459594399294283, w0=74.20000000000014, w1=13.630392274682018\n",
      "SubSGD iter. 236/499: loss=4.494804124421514, w0=73.50000000000014, w1=14.289570354376327\n",
      "SubSGD iter. 237/499: loss=4.474897730630071, w0=74.20000000000014, w1=15.19035268019733\n",
      "SubSGD iter. 238/499: loss=4.698470596220217, w0=74.90000000000015, w1=14.379290039084887\n",
      "SubSGD iter. 239/499: loss=4.678804424390922, w0=74.20000000000014, w1=15.413256612529114\n",
      "SubSGD iter. 240/499: loss=4.754125160408119, w0=73.50000000000014, w1=15.082211281636043\n",
      "SubSGD iter. 241/499: loss=4.60961468320689, w0=74.20000000000014, w1=15.494386794619576\n",
      "SubSGD iter. 242/499: loss=4.775855190281203, w0=74.90000000000015, w1=16.19111633274063\n",
      "SubSGD iter. 243/499: loss=5.113994147543615, w0=74.20000000000014, w1=15.48039508748223\n",
      "SubSGD iter. 244/499: loss=4.772074157450102, w0=74.90000000000015, w1=15.958320015849974\n",
      "SubSGD iter. 245/499: loss=5.03732193321876, w0=74.20000000000014, w1=16.40536464697823\n",
      "SubSGD iter. 246/499: loss=5.07105817855896, w0=73.50000000000014, w1=15.478140277383014\n",
      "SubSGD iter. 247/499: loss=4.709554604261893, w0=72.80000000000014, w1=15.843719847157027\n",
      "SubSGD iter. 248/499: loss=4.822393219882313, w0=72.10000000000014, w1=15.54265285028903\n",
      "SubSGD iter. 249/499: loss=4.804724216163775, w0=72.80000000000014, w1=15.853102115281498\n",
      "SubSGD iter. 250/499: loss=4.825380836125293, w0=72.10000000000014, w1=15.941617942591702\n",
      "SubSGD iter. 251/499: loss=4.924438397596368, w0=71.40000000000013, w1=14.48917938247735\n",
      "SubSGD iter. 252/499: loss=4.725380005539256, w0=72.10000000000014, w1=14.411282914044369\n",
      "SubSGD iter. 253/499: loss=4.571527485480285, w0=71.40000000000013, w1=13.59066658090091\n",
      "SubSGD iter. 254/499: loss=4.661413395699693, w0=70.70000000000013, w1=13.26114794267888\n",
      "SubSGD iter. 255/499: loss=4.88324227033384, w0=71.40000000000013, w1=13.665391207253055\n",
      "SubSGD iter. 256/499: loss=4.662017303739326, w0=72.10000000000014, w1=14.444103614694521\n",
      "SubSGD iter. 257/499: loss=4.575749931666447, w0=71.40000000000013, w1=15.042120972749583\n",
      "SubSGD iter. 258/499: loss=4.821128745339014, w0=70.70000000000013, w1=15.557754388390592\n",
      "SubSGD iter. 259/499: loss=5.14355275499048, w0=70.00000000000013, w1=15.25950359154002\n",
      "SubSGD iter. 260/499: loss=5.335331575190268, w0=70.70000000000013, w1=14.76194704573626\n",
      "SubSGD iter. 261/499: loss=4.974984611149566, w0=71.40000000000013, w1=13.056566260747047\n",
      "SubSGD iter. 262/499: loss=4.6821238309482265, w0=72.10000000000014, w1=13.211595096127063\n",
      "SubSGD iter. 263/499: loss=4.525854652604096, w0=71.40000000000013, w1=13.049967904285385\n",
      "SubSGD iter. 264/499: loss=4.682642646651339, w0=70.70000000000013, w1=12.227593926058477\n",
      "SubSGD iter. 265/499: loss=5.008449435406694, w0=70.00000000000013, w1=13.191807702890191\n",
      "SubSGD iter. 266/499: loss=5.164805121324968, w0=70.70000000000013, w1=14.536752241476647\n",
      "SubSGD iter. 267/499: loss=4.940961385505263, w0=71.40000000000013, w1=14.942505678158991\n",
      "SubSGD iter. 268/499: loss=4.800846600283768, w0=72.10000000000014, w1=14.658050781910022\n",
      "SubSGD iter. 269/499: loss=4.607475591791479, w0=72.80000000000014, w1=15.758705754742206\n",
      "SubSGD iter. 270/499: loss=4.79580921444564, w0=73.50000000000014, w1=15.558064280428468\n",
      "SubSGD iter. 271/499: loss=4.732193551008807, w0=74.20000000000014, w1=15.596382721672702\n",
      "SubSGD iter. 272/499: loss=4.804246725309045, w0=73.50000000000014, w1=15.977880848017247\n",
      "SubSGD iter. 273/499: loss=4.862037912379387, w0=72.80000000000014, w1=15.559207037504969\n",
      "SubSGD iter. 274/499: loss=4.737048383415726, w0=73.50000000000014, w1=15.427111286711957\n",
      "SubSGD iter. 275/499: loss=4.695474018600414, w0=74.20000000000014, w1=13.866545797400514\n",
      "SubSGD iter. 276/499: loss=4.503024886608415, w0=74.90000000000015, w1=14.077069585585795\n",
      "SubSGD iter. 277/499: loss=4.648224903623042, w0=74.20000000000014, w1=12.778587657049966\n",
      "SubSGD iter. 278/499: loss=4.540615506078696, w0=74.90000000000015, w1=13.507012392678945\n",
      "SubSGD iter. 279/499: loss=4.628689939362996, w0=74.20000000000014, w1=14.866439210982433\n",
      "SubSGD iter. 280/499: loss=4.62838171775383, w0=73.50000000000014, w1=14.311081697077583\n",
      "SubSGD iter. 281/499: loss=4.477419522302163, w0=74.20000000000014, w1=13.041606847419146\n",
      "SubSGD iter. 282/499: loss=4.513838624908295, w0=73.50000000000014, w1=12.756486852961359\n",
      "SubSGD iter. 283/499: loss=4.48133062418033, w0=74.20000000000014, w1=12.606374712826407\n",
      "SubSGD iter. 284/499: loss=4.563911519430311, w0=73.50000000000014, w1=12.665286303104436\n",
      "SubSGD iter. 285/499: loss=4.4945166380048205, w0=74.20000000000014, w1=13.084755222432243\n",
      "SubSGD iter. 286/499: loss=4.5106092916893195, w0=73.50000000000014, w1=12.809765994091146\n",
      "SubSGD iter. 287/499: loss=4.474256176746973, w0=74.20000000000014, w1=13.211131509856529\n",
      "SubSGD iter. 288/499: loss=4.502627810293407, w0=73.50000000000014, w1=13.643409359019781\n",
      "SubSGD iter. 289/499: loss=4.430466291081825, w0=72.80000000000014, w1=14.325944621244183\n",
      "SubSGD iter. 290/499: loss=4.4844701127414455, w0=72.10000000000014, w1=14.51375208866692\n",
      "SubSGD iter. 291/499: loss=4.585297415587545, w0=71.40000000000013, w1=13.587455740915626\n",
      "SubSGD iter. 292/499: loss=4.661407607957668, w0=70.70000000000013, w1=13.622918028111544\n",
      "SubSGD iter. 293/499: loss=4.8757795691203505, w0=71.40000000000013, w1=14.111803617659884\n",
      "SubSGD iter. 294/499: loss=4.683946626581596, w0=72.10000000000014, w1=13.909138114048151\n",
      "SubSGD iter. 295/499: loss=4.524941480134793, w0=71.40000000000013, w1=14.042150664730476\n",
      "SubSGD iter. 296/499: loss=4.678480123987085, w0=72.10000000000014, w1=12.725922449126463\n",
      "SubSGD iter. 297/499: loss=4.572328151458361, w0=72.80000000000014, w1=13.528910180944104\n",
      "SubSGD iter. 298/499: loss=4.438057622402312, w0=73.50000000000014, w1=13.988412250930914\n",
      "SubSGD iter. 299/499: loss=4.446060121760542, w0=72.80000000000014, w1=14.475447438369926\n",
      "SubSGD iter. 300/499: loss=4.504238160969257, w0=72.10000000000014, w1=13.037277856991416\n",
      "SubSGD iter. 301/499: loss=4.538410388699414, w0=72.80000000000014, w1=13.227315962310088\n",
      "SubSGD iter. 302/499: loss=4.4463949067438495, w0=73.50000000000014, w1=12.86326712021162\n",
      "SubSGD iter. 303/499: loss=4.467777753137487, w0=72.80000000000014, w1=12.360349617254947\n",
      "SubSGD iter. 304/499: loss=4.55665616558928, w0=72.10000000000014, w1=11.299628354131977\n",
      "SubSGD iter. 305/499: loss=4.928265281903092, w0=72.80000000000014, w1=10.263874577902058\n",
      "SubSGD iter. 306/499: loss=5.289881228192057, w0=73.50000000000014, w1=10.895923456171982\n",
      "SubSGD iter. 307/499: loss=4.995549281895798, w0=74.20000000000014, w1=11.360252735979518\n",
      "SubSGD iter. 308/499: loss=4.872197019408186, w0=74.90000000000015, w1=10.520171515982605\n",
      "SubSGD iter. 309/499: loss=5.301574612008131, w0=74.20000000000014, w1=10.785637049765171\n",
      "SubSGD iter. 310/499: loss=5.087569902059233, w0=74.90000000000015, w1=11.678826437561636\n",
      "SubSGD iter. 311/499: loss=4.890110946639231, w0=74.20000000000014, w1=11.704197873799691\n",
      "SubSGD iter. 312/499: loss=4.763904756320524, w0=74.90000000000015, w1=11.699585427206099\n",
      "SubSGD iter. 313/499: loss=4.884340810847926, w0=75.60000000000015, w1=10.811858631469207\n",
      "SubSGD iter. 314/499: loss=5.3425745867063394, w0=76.30000000000015, w1=11.992770364527065\n",
      "SubSGD iter. 315/499: loss=5.244697911467573, w0=75.60000000000015, w1=11.65194509068679\n",
      "SubSGD iter. 316/499: loss=5.0763234491757405, w0=76.30000000000015, w1=11.782681401970935\n",
      "SubSGD iter. 317/499: loss=5.288186903322605, w0=77.00000000000016, w1=11.89350503037433\n",
      "SubSGD iter. 318/499: loss=5.561908636701521, w0=77.70000000000016, w1=12.122157076749609\n",
      "SubSGD iter. 319/499: loss=5.876065392673647, w0=77.00000000000016, w1=10.606933433742663\n",
      "SubSGD iter. 320/499: loss=5.895375322429066, w0=76.30000000000015, w1=11.591973399341763\n",
      "SubSGD iter. 321/499: loss=5.332079096058399, w0=77.00000000000016, w1=10.8734532926312\n",
      "SubSGD iter. 322/499: loss=5.811324026001365, w0=76.30000000000015, w1=12.325546130095525\n",
      "SubSGD iter. 323/499: loss=5.1861053140922, w0=75.60000000000015, w1=11.786220113165228\n",
      "SubSGD iter. 324/499: loss=5.042709446130843, w0=74.90000000000015, w1=12.648193551674918\n",
      "SubSGD iter. 325/499: loss=4.689580501039254, w0=75.60000000000015, w1=12.97613591897054\n",
      "SubSGD iter. 326/499: loss=4.854409210604621, w0=74.90000000000015, w1=11.95807587366034\n",
      "SubSGD iter. 327/499: loss=4.817531157311738, w0=74.20000000000014, w1=11.509626017724438\n",
      "SubSGD iter. 328/499: loss=4.823379493065527, w0=74.90000000000015, w1=12.182522568116386\n",
      "SubSGD iter. 329/499: loss=4.767981269164966, w0=75.60000000000015, w1=11.723507069934668\n",
      "SubSGD iter. 330/499: loss=5.058120486962789, w0=76.30000000000015, w1=12.199835640973536\n",
      "SubSGD iter. 331/499: loss=5.2067307171770025, w0=77.00000000000016, w1=12.43921198869187\n",
      "SubSGD iter. 332/499: loss=5.476077911593651, w0=76.30000000000015, w1=11.384264329907271\n",
      "SubSGD iter. 333/499: loss=5.385122468888255, w0=77.00000000000016, w1=12.507544718946802\n",
      "SubSGD iter. 334/499: loss=5.467826391805911, w0=76.30000000000015, w1=12.966830185613405\n",
      "SubSGD iter. 335/499: loss=5.112892653561315, w0=75.60000000000015, w1=12.312495136745587\n",
      "SubSGD iter. 336/499: loss=4.937110610960269, w0=74.90000000000015, w1=13.274635095906953\n",
      "SubSGD iter. 337/499: loss=4.6344209926792335, w0=75.60000000000015, w1=13.853572241252035\n",
      "SubSGD iter. 338/499: loss=4.834476429579091, w0=74.90000000000015, w1=13.403688039756215\n",
      "SubSGD iter. 339/499: loss=4.630275512388913, w0=75.60000000000015, w1=14.279729845496455\n",
      "SubSGD iter. 340/499: loss=4.86397640409067, w0=74.90000000000015, w1=14.282607878841667\n",
      "SubSGD iter. 341/499: loss=4.667454325549455, w0=74.20000000000014, w1=14.04384044653341\n",
      "SubSGD iter. 342/499: loss=4.514770275998799, w0=74.90000000000015, w1=13.182470914103446\n",
      "SubSGD iter. 343/499: loss=4.638928453982494, w0=74.20000000000014, w1=13.399872794230168\n",
      "SubSGD iter. 344/499: loss=4.495810881014152, w0=73.50000000000014, w1=13.631066066145596\n",
      "SubSGD iter. 345/499: loss=4.430258293315736, w0=72.80000000000014, w1=13.730687061501847\n",
      "SubSGD iter. 346/499: loss=4.440214500673929, w0=73.50000000000014, w1=14.228656891823128\n",
      "SubSGD iter. 347/499: loss=4.468010683752008, w0=72.80000000000014, w1=13.253063305287405\n",
      "SubSGD iter. 348/499: loss=4.445094952688364, w0=73.50000000000014, w1=13.858002387598612\n",
      "SubSGD iter. 349/499: loss=4.437970577694225, w0=74.20000000000014, w1=12.172528688445711\n",
      "SubSGD iter. 350/499: loss=4.644370399951791, w0=74.90000000000015, w1=10.675194869495654\n",
      "SubSGD iter. 351/499: loss=5.2367693566947375, w0=75.60000000000015, w1=10.69056527626085\n",
      "SubSGD iter. 352/499: loss=5.388254253214046, w0=74.90000000000015, w1=11.605907932900584\n",
      "SubSGD iter. 353/499: loss=4.910808577254123, w0=74.20000000000014, w1=12.045163149547403\n",
      "SubSGD iter. 354/499: loss=4.674009982012332, w0=74.90000000000015, w1=12.191677030799408\n",
      "SubSGD iter. 355/499: loss=4.766120309610863, w0=74.20000000000014, w1=10.708211843721498\n",
      "SubSGD iter. 356/499: loss=5.119556605470616, w0=74.90000000000015, w1=11.089932692181426\n",
      "SubSGD iter. 357/499: loss=5.077204187378085, w0=75.60000000000015, w1=11.21090854090665\n",
      "SubSGD iter. 358/499: loss=5.2039483315721515, w0=76.30000000000015, w1=11.83570736130001\n",
      "SubSGD iter. 359/499: loss=5.276813233831148, w0=77.00000000000016, w1=11.764321129662227\n",
      "SubSGD iter. 360/499: loss=5.587347529194855, w0=76.30000000000015, w1=12.622837583500724\n",
      "SubSGD iter. 361/499: loss=5.1457656238274225, w0=75.60000000000015, w1=11.11883906367836\n",
      "SubSGD iter. 362/499: loss=5.234037105890925, w0=76.30000000000015, w1=11.358628672245544\n",
      "SubSGD iter. 363/499: loss=5.392005591811565, w0=75.60000000000015, w1=11.307539488209098\n",
      "SubSGD iter. 364/499: loss=5.173588545868631, w0=74.90000000000015, w1=11.85274346324494\n",
      "SubSGD iter. 365/499: loss=4.8434975556436335, w0=75.60000000000015, w1=12.588013885271833\n",
      "SubSGD iter. 366/499: loss=4.896231958203685, w0=74.90000000000015, w1=11.597083818452617\n",
      "SubSGD iter. 367/499: loss=4.91334839201869, w0=75.60000000000015, w1=11.837954217107693\n",
      "SubSGD iter. 368/499: loss=5.030517153772708, w0=74.90000000000015, w1=12.922462293751073\n",
      "SubSGD iter. 369/499: loss=4.658660809145859, w0=75.60000000000015, w1=13.031145182191151\n",
      "SubSGD iter. 370/499: loss=4.850100621307233, w0=76.30000000000015, w1=12.272137830762912\n",
      "SubSGD iter. 371/499: loss=5.194601994516081, w0=77.00000000000016, w1=11.607216498992091\n",
      "SubSGD iter. 372/499: loss=5.620351446643188, w0=76.30000000000015, w1=11.633605947181527\n",
      "SubSGD iter. 373/499: loss=5.322085593517905, w0=75.60000000000015, w1=12.236126925165756\n",
      "SubSGD iter. 374/499: loss=4.950138865113409, w0=74.90000000000015, w1=11.340728365680727\n",
      "SubSGD iter. 375/499: loss=4.991698534891324, w0=74.20000000000014, w1=11.790136444715165\n",
      "SubSGD iter. 376/499: loss=4.739659232820512, w0=73.50000000000014, w1=12.25802431438428\n",
      "SubSGD iter. 377/499: loss=4.5692166286172275, w0=72.80000000000014, w1=12.34505881822615\n",
      "SubSGD iter. 378/499: loss=4.5597667661308225, w0=72.10000000000014, w1=11.978109041575683\n",
      "SubSGD iter. 379/499: loss=4.720608902610038, w0=71.40000000000013, w1=12.628768481405196\n",
      "SubSGD iter. 380/499: loss=4.731915763348099, w0=70.70000000000013, w1=13.763679732116746\n",
      "SubSGD iter. 381/499: loss=4.878196519810964, w0=70.00000000000013, w1=13.979222803003584\n",
      "SubSGD iter. 382/499: loss=5.165438598159103, w0=70.70000000000013, w1=13.52542679759577\n",
      "SubSGD iter. 383/499: loss=4.876058077916609, w0=71.40000000000013, w1=13.344539221177827\n",
      "SubSGD iter. 384/499: loss=4.665621231925365, w0=72.10000000000014, w1=14.401193550960969\n",
      "SubSGD iter. 385/499: loss=4.570251408594685, w0=72.80000000000014, w1=15.144264951707607\n",
      "SubSGD iter. 386/499: loss=4.630477697115529, w0=72.10000000000014, w1=14.285209397463927\n",
      "SubSGD iter. 387/499: loss=4.556544687023779, w0=72.80000000000014, w1=13.929604222689589\n",
      "SubSGD iter. 388/499: loss=4.448780134249432, w0=72.10000000000014, w1=13.583598518410502\n",
      "SubSGD iter. 389/499: loss=4.515605860634059, w0=72.80000000000014, w1=13.352797987068406\n",
      "SubSGD iter. 390/499: loss=4.44108635356587, w0=73.50000000000014, w1=14.017247784205402\n",
      "SubSGD iter. 391/499: loss=4.448203627291651, w0=74.20000000000014, w1=14.344141821397479\n",
      "SubSGD iter. 392/499: loss=4.545080905252302, w0=73.50000000000014, w1=14.645611493978986\n",
      "SubSGD iter. 393/499: loss=4.524585291640596, w0=74.20000000000014, w1=14.270193178017475\n",
      "SubSGD iter. 394/499: loss=4.536415703953571, w0=73.50000000000014, w1=14.749766336194124\n",
      "SubSGD iter. 395/499: loss=4.542501931374869, w0=74.20000000000014, w1=14.280733513876092\n",
      "SubSGD iter. 396/499: loss=4.537601370028789, w0=73.50000000000014, w1=12.56109249022028\n",
      "SubSGD iter. 397/499: loss=4.511351660250542, w0=74.20000000000014, w1=11.498831534368044\n",
      "SubSGD iter. 398/499: loss=4.826833007209117, w0=74.90000000000015, w1=11.359671077206553\n",
      "SubSGD iter. 399/499: loss=4.98556838685906, w0=74.20000000000014, w1=11.596488393917966\n",
      "SubSGD iter. 400/499: loss=4.79618228554096, w0=74.90000000000015, w1=12.054288244616233\n",
      "SubSGD iter. 401/499: loss=4.795295861619877, w0=74.20000000000014, w1=11.564617605635197\n",
      "SubSGD iter. 402/499: loss=4.806039445990868, w0=74.90000000000015, w1=11.705192380204975\n",
      "SubSGD iter. 403/499: loss=4.882798686453454, w0=75.60000000000015, w1=11.433957953880565\n",
      "SubSGD iter. 404/499: loss=5.135897718738915, w0=76.30000000000015, w1=11.948474273719452\n",
      "SubSGD iter. 405/499: loss=5.253500174171091, w0=77.00000000000016, w1=11.427369529143503\n",
      "SubSGD iter. 406/499: loss=5.661499479767473, w0=77.70000000000016, w1=11.130230195560891\n",
      "SubSGD iter. 407/499: loss=6.067897629773534, w0=77.00000000000016, w1=11.941187431407817\n",
      "SubSGD iter. 408/499: loss=5.552983340018718, w0=76.30000000000015, w1=11.497765344228828\n",
      "SubSGD iter. 409/499: loss=5.355558430317172, w0=75.60000000000015, w1=11.84145638320384\n",
      "SubSGD iter. 410/499: loss=5.029720117899851, w0=74.90000000000015, w1=12.060400271533918\n",
      "SubSGD iter. 411/499: loss=4.793936826817534, w0=74.20000000000014, w1=12.711522125092682\n",
      "SubSGD iter. 412/499: loss=4.549118518382077, w0=73.50000000000014, w1=12.280376314143718\n",
      "SubSGD iter. 413/499: loss=4.5645416918692145, w0=72.80000000000014, w1=12.869787244238024\n",
      "SubSGD iter. 414/499: loss=4.4760257105179155, w0=73.50000000000014, w1=12.355253740274017\n",
      "SubSGD iter. 415/499: loss=4.549314809519147, w0=72.80000000000014, w1=12.30539939479271\n",
      "SubSGD iter. 416/499: loss=4.568014658177088, w0=72.10000000000014, w1=12.078887334407186\n",
      "SubSGD iter. 417/499: loss=4.695305686481821, w0=72.80000000000014, w1=12.508277793965084\n",
      "SubSGD iter. 418/499: loss=4.528980859440427, w0=73.50000000000014, w1=13.09511713808678\n",
      "SubSGD iter. 419/499: loss=4.445756619744655, w0=72.80000000000014, w1=13.54735988294778\n",
      "SubSGD iter. 420/499: loss=4.438015182331086, w0=72.10000000000014, w1=13.287268357342478\n",
      "SubSGD iter. 421/499: loss=4.521845906865363, w0=72.80000000000014, w1=14.441872998605518\n",
      "SubSGD iter. 422/499: loss=4.499516443607043, w0=73.50000000000014, w1=13.100732109656184\n",
      "SubSGD iter. 423/499: loss=4.4453514646493355, w0=72.80000000000014, w1=13.604185906214958\n",
      "SubSGD iter. 424/499: loss=4.438151650727186, w0=72.10000000000014, w1=14.408090734650747\n",
      "SubSGD iter. 425/499: loss=4.571121445785349, w0=72.80000000000014, w1=14.949537097451472\n",
      "SubSGD iter. 426/499: loss=4.587819447047703, w0=73.50000000000014, w1=14.186490490513897\n",
      "SubSGD iter. 427/499: loss=4.463515667350939, w0=74.20000000000014, w1=15.039023948491712\n",
      "SubSGD iter. 428/499: loss=4.664320919184117, w0=73.50000000000014, w1=15.515620309539669\n",
      "SubSGD iter. 429/499: loss=4.720086663039475, w0=74.20000000000014, w1=15.666974567586628\n",
      "SubSGD iter. 430/499: loss=4.824789228897, w0=74.90000000000015, w1=14.471292812166908\n",
      "SubSGD iter. 431/499: loss=4.690958184368738, w0=74.20000000000014, w1=15.50498856313402\n",
      "SubSGD iter. 432/499: loss=4.778735620829225, w0=74.90000000000015, w1=15.305356918395985\n",
      "SubSGD iter. 433/499: loss=4.852328031148755, w0=74.20000000000014, w1=16.06711684039684\n",
      "SubSGD iter. 434/499: loss=4.95098536035386, w0=74.90000000000015, w1=16.15714877116387\n",
      "SubSGD iter. 435/499: loss=5.1024914666341665, w0=74.20000000000014, w1=16.178976269830308\n",
      "SubSGD iter. 436/499: loss=4.989406614140364, w0=73.50000000000014, w1=15.421187637860756\n",
      "SubSGD iter. 437/499: loss=4.693858157988451, w0=74.20000000000014, w1=14.280585883532074\n",
      "SubSGD iter. 438/499: loss=4.537584585953308, w0=74.90000000000015, w1=14.742922606534203\n",
      "SubSGD iter. 439/499: loss=4.733515630810673, w0=75.60000000000015, w1=15.15470516014737\n",
      "SubSGD iter. 440/499: loss=5.005769229108031, w0=74.90000000000015, w1=14.327042943503857\n",
      "SubSGD iter. 441/499: loss=4.6725378357553655, w0=74.20000000000014, w1=13.39883238657767\n",
      "SubSGD iter. 442/499: loss=4.495835750559514, w0=74.90000000000015, w1=13.84101461862266\n",
      "SubSGD iter. 443/499: loss=4.634604247342971, w0=74.20000000000014, w1=12.921836353055166\n",
      "SubSGD iter. 444/499: loss=4.524661856664827, w0=73.50000000000014, w1=13.283610192246869\n",
      "SubSGD iter. 445/499: loss=4.434481651932469, w0=72.80000000000014, w1=13.757570074117263\n",
      "SubSGD iter. 446/499: loss=4.441021096892615, w0=72.10000000000014, w1=13.793249581886558\n",
      "SubSGD iter. 447/499: loss=4.519730742010538, w0=71.40000000000013, w1=14.519313795924374\n",
      "SubSGD iter. 448/499: loss=4.7295406967578835, w0=72.10000000000014, w1=14.635124213903573\n",
      "SubSGD iter. 449/499: loss=4.603756323484639, w0=71.40000000000013, w1=14.990067903439359\n",
      "SubSGD iter. 450/499: loss=4.810377607799157, w0=70.70000000000013, w1=15.167110826097067\n",
      "SubSGD iter. 451/499: loss=5.051805613613029, w0=71.40000000000013, w1=14.254398140533498\n",
      "SubSGD iter. 452/499: loss=4.697453465664224, w0=72.10000000000014, w1=14.423591258898817\n",
      "SubSGD iter. 453/499: loss=4.573097338409029, w0=72.80000000000014, w1=14.142295527318414\n",
      "SubSGD iter. 454/499: loss=4.4649099539311194, w0=73.50000000000014, w1=14.207933792680711\n",
      "SubSGD iter. 455/499: loss=4.465765185640099, w0=72.80000000000014, w1=14.847619091191017\n",
      "SubSGD iter. 456/499: loss=4.567486090704707, w0=73.50000000000014, w1=15.144169078735775\n",
      "SubSGD iter. 457/499: loss=4.623907394480291, w0=72.80000000000014, w1=14.434259632401501\n",
      "SubSGD iter. 458/499: loss=4.498465824524937, w0=73.50000000000014, w1=15.732299690856479\n",
      "SubSGD iter. 459/499: loss=4.783911505574243, w0=72.80000000000014, w1=15.056129702724167\n",
      "SubSGD iter. 460/499: loss=4.610500788907753, w0=73.50000000000014, w1=14.52357135404038\n",
      "SubSGD iter. 461/499: loss=4.505742014301147, w0=72.80000000000014, w1=15.054844035946935\n",
      "SubSGD iter. 462/499: loss=4.61021790620956, w0=73.50000000000014, w1=15.236047710079113\n",
      "SubSGD iter. 463/499: loss=4.645977289825643, w0=74.20000000000014, w1=14.714394677043616\n",
      "SubSGD iter. 464/499: loss=4.6003808361859, w0=73.50000000000014, w1=14.814534192782805\n",
      "SubSGD iter. 465/499: loss=4.554418053820071, w0=74.20000000000014, w1=15.31434771544724\n",
      "SubSGD iter. 466/499: loss=4.728572315482161, w0=74.90000000000015, w1=14.065374208591628\n",
      "SubSGD iter. 467/499: loss=4.647352025903643, w0=75.60000000000015, w1=12.837154888331803\n",
      "SubSGD iter. 468/499: loss=4.867085842333713, w0=74.90000000000015, w1=13.510344315926737\n",
      "SubSGD iter. 469/499: loss=4.628665953657903, w0=74.20000000000014, w1=12.532446418073937\n",
      "SubSGD iter. 470/499: loss=4.575331298365585, w0=74.90000000000015, w1=12.834956822237022\n",
      "SubSGD iter. 471/499: loss=4.667269244966911, w0=74.20000000000014, w1=12.665983842512116\n",
      "SubSGD iter. 472/499: loss=4.555291111492151, w0=74.90000000000015, w1=13.680586102196195\n",
      "SubSGD iter. 473/499: loss=4.629725892093948, w0=75.60000000000015, w1=13.074189355834456\n",
      "SubSGD iter. 474/499: loss=4.847040122539268, w0=76.30000000000015, w1=14.231132164337588\n",
      "SubSGD iter. 475/499: loss=5.119571677730845, w0=75.60000000000015, w1=15.111373358186219\n",
      "SubSGD iter. 476/499: loss=4.996248688993898, w0=74.90000000000015, w1=16.454197307903137\n",
      "SubSGD iter. 477/499: loss=5.206438881939657, w0=74.20000000000014, w1=17.046976877597185\n",
      "SubSGD iter. 478/499: loss=5.327772243474959, w0=73.50000000000014, w1=17.170121114438988\n",
      "SubSGD iter. 479/499: loss=5.3283544607194, w0=72.80000000000014, w1=16.789517721132317\n",
      "SubSGD iter. 480/499: loss=5.169436063813656, w0=73.50000000000014, w1=17.26874729086544\n",
      "SubSGD iter. 481/499: loss=5.372239648968152, w0=72.80000000000014, w1=17.03943229291192\n",
      "SubSGD iter. 482/499: loss=5.274513815439082, w0=72.10000000000014, w1=15.748494389619847\n",
      "SubSGD iter. 483/499: loss=4.864332509265706, w0=72.80000000000014, w1=15.371349525545245\n",
      "SubSGD iter. 484/499: loss=4.686078682973889, w0=72.10000000000014, w1=15.413456690954991\n",
      "SubSGD iter. 485/499: loss=4.769770003076392, w0=72.80000000000014, w1=15.946504102919373\n",
      "SubSGD iter. 486/499: loss=4.8556710191131245, w0=73.50000000000014, w1=15.247120504344563\n",
      "SubSGD iter. 487/499: loss=4.648705301722832, w0=74.20000000000014, w1=14.952634479635874\n",
      "SubSGD iter. 488/499: loss=4.645876811285371, w0=73.50000000000014, w1=15.610029043350742\n",
      "SubSGD iter. 489/499: loss=4.747312658121497, w0=72.80000000000014, w1=15.648224854076478\n",
      "SubSGD iter. 490/499: loss=4.7625648281664095, w0=72.10000000000014, w1=16.100291854623475\n",
      "SubSGD iter. 491/499: loss=4.976717149663633, w0=71.40000000000013, w1=15.96294911396725\n",
      "SubSGD iter. 492/499: loss=5.06348647620552, w0=70.70000000000013, w1=16.66080861796909\n",
      "SubSGD iter. 493/499: loss=5.49034223139009, w0=71.40000000000013, w1=15.987306404579238\n",
      "SubSGD iter. 494/499: loss=5.0711538759288635, w0=72.10000000000014, w1=16.543651452120702\n",
      "SubSGD iter. 495/499: loss=5.13592628446002, w0=71.40000000000013, w1=16.16810054939787\n",
      "SubSGD iter. 496/499: loss=5.1295055734219615, w0=72.10000000000014, w1=16.790809594399757\n",
      "SubSGD iter. 497/499: loss=5.233026250186403, w0=71.40000000000013, w1=17.943387302589798\n",
      "SubSGD iter. 498/499: loss=5.870001600833195, w0=70.70000000000013, w1=18.032949573785107\n",
      "SubSGD iter. 499/499: loss=6.081129847013394, w0=71.40000000000013, w1=17.279144704866148\n",
      "SubSGD: execution time=0.056 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcf57f3cd564d80acf87cfdaa2c6b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
